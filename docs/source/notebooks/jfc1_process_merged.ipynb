{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing multiple CRNS probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the time being, this notebook is a sandbox.** Step by step, we will migrate mature functions to the actual `cosmicsense` library.\n",
    "\n",
    "For a detailed explanation of prepprocessing steps, please refer to [from_n_to_theta.ipynb](https://cosmicsense.readthedocs.io/en/latest/notebooks/n_to_theta.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from bokeh.plotting import figure, show, save, output_file, output_notebook\n",
    "from bokeh.palettes import Spectral11, colorblind, Inferno, BuGn, brewer\n",
    "from bokeh.models import LinearAxis, HoverTool, value, LabelSet, Legend, ColumnDataSource,LinearColorMapper,BasicTicker, PrintfTickFormatter, ColorBar\n",
    "from bokeh.palettes import Spectral3\n",
    "import bokeh.palettes\n",
    "from bokeh.embed import components\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import cosmicsense as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('once', RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# Display figures inline\n",
    "%matplotlib inline\n",
    "# Display figures as interactive (requires kernel restart)\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRNS records from three probes with different temporal coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = [1,2,3,4,16,17,18,19,23,24]\n",
    "ids = [1, 2, 3, 4, 5, 6, 7, 8, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25]\n",
    "\n",
    "#fpath = \"/home/maik/b2drop/cosmicsense/inbox/fendt/timeseries/crns/JFC-1/\"\n",
    "fpath = \"/media/x/cosmicsense/data/fendt/crns/\"\n",
    "htmlfile = \"/media/x/cosmicsense/data/fendt/crns/crnsmerged.html\"\n",
    "plot_width = 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 2019-05-09 09:59:00 to 2019-06-13 07:19:00\n",
      "2: 2019-05-07 11:02:13 to 2019-06-13 05:40:00\n",
      "3: 2019-05-07 08:37:25 to 2019-06-13 09:26:00\n",
      "4: 2019-05-07 15:21:29 to 2019-06-13 05:39:00\n",
      "5: 2019-05-03 08:53:04 to 2019-06-07 08:37:00\n",
      "6: 2019-05-03 09:34:48 to 2019-06-07 08:41:00\n",
      "7: 2019-05-13 14:54:00 to 2019-06-07 09:40:00\n",
      "8: 2019-05-01 00:46:00 to 2019-06-07 08:46:00\n",
      "14: 2019-05-08 06:45:44 to 2019-05-31 09:19:00\n",
      "16: 2019-05-14 13:45:00 to 2019-06-12 16:31:00\n",
      "17: 2019-05-15 14:14:46 to 2019-06-12 14:26:00\n",
      "18: 2019-05-14 10:16:06 to 2019-06-06 14:35:00\n",
      "19: 2019-05-14 12:45:49 to 2019-06-13 03:00:00\n",
      "21: 2019-05-13 13:38:38 to 2019-06-10 20:56:00\n",
      "22: 2019-05-13 15:24:44 to 2019-06-10 21:18:00\n",
      "23: 2019-05-15 15:55:32 to 2019-06-13 04:19:00\n",
      "24: 2019-05-15 15:09:58 to 2019-06-12 15:19:00\n",
      "25: 2019-05-14 10:02:00 to 2019-06-07 08:41:00\n"
     ]
    }
   ],
   "source": [
    "crns = {}\n",
    "for id in ids:\n",
    "    df = pd.read_csv(path.join(fpath, \"%d/%d_CRNS_merge.txt\" % (id, id)), sep=\"\\t\")\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    df = df.set_index(\"datetime\")\n",
    "    if id==4:\n",
    "        df[\"cph1\"] = (df.counts1 + df.counts2) / cs.conv.s_to_h(df.nsecs1)\n",
    "    else:\n",
    "        df[\"cph1\"] = df.counts1 / cs.conv.s_to_h(df.nsecs1)\n",
    "        try:\n",
    "            df[\"cph2\"] = df.counts2 / cs.conv.s_to_h(df.nsecs2)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    print(id, end=\": \")\n",
    "    print(\"%s to %s\" % (df.index[0], df.index[-1]) )\n",
    "    crns[id] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dtime = np.min([crns[key].index[0] for key in crns.keys()])\n",
    "max_dtime = np.max([crns[key].index[-1] for key in crns.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter spurious signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some probes are affected by spurious count rates, presumably due to excess voltage from the solar panels in cases of intense insolation. In order to keep useful parts of the signal, a pragmatic/heuristic filtering approach is applied which could be refined later:\n",
    "\n",
    "1. Remove entirely unrealisticly count rates for specific probes (`mincph`, `maxcph`)\n",
    "2. Remove count rates from spuriously short count intervals (`mininterv`)\n",
    "3. After that, there are still spurious count rates.In order to detect these, we compute the maximum count rates over periods of six hours, and then apply a 24-hour-median filter to these 6-hour-maxima. That way, we try to truncate spurios peaks. In order to prevent too aggressive filtering, we add a `buffer` of the median count rates over a period of 24 hours.\n",
    "4. We then interpolate this upper limit filter to the original timestamp values and use it to remove high values.\n",
    "5. We remove unrealisticly low count rates (`mincph`), and than apply the same approach (points 3-4) to eliminate spuriously small values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars =  {\n",
    "    1: {\"mincph\": 500, \"maxcph\": 1000, \"mininterv\": -9999, \"type\": \"CRS 2000-B, Scn.\", \"lut\": \"forest/meadow\"},\n",
    "    2: {\"mincph\": 500, \"maxcph\": 1100, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    3: {\"mincph\": 500, \"maxcph\": 1100, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    4: {\"mincph\": 6000, \"maxcph\": 9500, \"mininterv\": -9999, \"type\": \"Lab C\", \"lut\": \"meadow\"},\n",
    "    5: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    6: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow, forest close\"},\n",
    "    7: {\"mincph\": 1000, \"maxcph\": 1700, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    8: {\"mincph\": 1300, \"maxcph\": 2500, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    14: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 2000\", \"lut\": \"forest\"},\n",
    "    16: {\"mincph\": 1500, \"maxcph\": 2500, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    17: {\"mincph\": 1400, \"maxcph\": 2400, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    18: {\"mincph\": 500, \"maxcph\": 1000, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    19: {\"mincph\": 1300, \"maxcph\": 2300, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"forest\"},\n",
    "    21: {\"mincph\": 1400, \"maxcph\": 2300, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow, forest close\"},\n",
    "    22: {\"mincph\": 1100, \"maxcph\": 2100, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"forest\"},\n",
    "    23: {\"mincph\": 1200, \"maxcph\": 2200, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow, peat\"},\n",
    "    24: {\"mincph\": 1600, \"maxcph\": 2600, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    25: {\"mincph\": 900, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    \n",
    "}\n",
    "\n",
    "buffer = 0.075\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    x = crns[key].cph1.copy()\n",
    "    if not key==1:\n",
    "        x[x > pars[key][\"maxcph\"]] = np.nan\n",
    "    x[x < pars[key][\"mincph\"]] = np.nan\n",
    "    x[crns[key].nsecs1 < pars[key][\"mininterv\"]] = np.nan\n",
    "    median24 = x.resample(\"24H\").median()\n",
    "    # Maxfilter\n",
    "    max6 = x.resample(\"6H\").max()\n",
    "    median24max6 = max6.resample(\"24H\").median()\n",
    "    maxfilter = np.array(median24max6 + buffer * median24)\n",
    "    # Minfilter\n",
    "    min6 = x.resample(\"6H\").min()\n",
    "    median24min6 = min6.resample(\"24H\").median()\n",
    "    minfilter = np.array(median24min6 - buffer * median24)    \n",
    "    # Resample filter to original time stamps\n",
    "    crns[key][\"cph1_maxfilter\"] = np.interp(x.index, median24.index, maxfilter)\n",
    "    crns[key][\"cph1_minfilter\"] = np.interp(x.index, median24.index, minfilter)\n",
    "    # Fill gaps\n",
    "    crns[key][\"cph1_maxfilter\"] = crns[key].cph1_maxfilter.interpolate()\n",
    "    crns[key][\"cph1_minfilter\"] = crns[key].cph1_minfilter.interpolate()\n",
    "    # Apply filter\n",
    "    crns[key][\"cph1_filtered\"] = x\n",
    "    if not key==1:\n",
    "        crns[key].loc[crns[key].cph1 > crns[key].cph1_maxfilter, \"cph1_filtered\"] = np.nan\n",
    "        crns[key].loc[crns[key].cph1 < crns[key].cph1_minfilter, \"cph1_filtered\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size'   : 12})\n",
    "titles = [\"CRNS #1\", \"CRNS #2\", \"CRNS #3\", \"NMDB station counts\", \"Barometric pressure\"]\n",
    "fig, ax = plt.subplots(nrows=len(crns), figsize=(12,40))\n",
    "\n",
    "xlim = min_dtime, max_dtime\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1, linestyle=\"None\", marker=\".\", ms=1, color=\"red\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_maxfilter, linestyle=\"-\", ms=0, color=\"orange\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_minfilter, linestyle=\"-\", ms=0, color=\"orange\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_filtered, linestyle=\"None\", marker=\".\", ms=1, color=\"black\")\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "    ax[i].set_ylabel(\"cph\")\n",
    "    ax[i].set_xlim(xlim)\n",
    "    #ax[i].set_ylim(pars[key][\"mincph\"], pars[key][\"maxcph\"]+200)\n",
    "    ax[i].grid()\n",
    "    #ax[i].legend()    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to a common time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dtime, max_dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrange6 = pd.date_range('2019-05-01 00:00:00', max_dtime, freq=\"6H\")\n",
    "dtrange24 = pd.date_range('2019-05-01 00:00:00', max_dtime, freq=\"24H\")\n",
    "crns6h = pd.DataFrame({}, index=dtrange6)\n",
    "crns24h = pd.DataFrame({}, index=dtrange24)\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    crns6h = pd.merge(crns6h, crns[key].cph1_filtered.resample(\"6H\").mean(), \n",
    "                      how=\"left\", left_index=True, right_index=True)\n",
    "    crns6h[key] = crns6h.cph1_filtered\n",
    "    crns6h = crns6h.drop(\"cph1_filtered\", axis=1)\n",
    "    # 24 h\n",
    "    crns24h = pd.merge(crns24h, crns[key].cph1_filtered.resample(\"24H\").mean(), \n",
    "                      how=\"left\", left_index=True, right_index=True)\n",
    "    crns24h[key] = crns24h.cph1_filtered\n",
    "    crns24h = crns24h.drop(\"cph1_filtered\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfile_merge = \"/media/x/cosmicsense/git/misc/fendt/merged_crns.html\"\n",
    "TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "colors=bokeh.palettes.Category20_20#bokeh.palettes.Set3_12 + bokeh.palettes.Set2_6\n",
    "\n",
    "plts = []\n",
    "\n",
    "from bokeh.models import Span\n",
    "from bokeh.models import Label\n",
    "\n",
    "for i, id in enumerate(ids):\n",
    "    p = figure(title=\"Counts/hour (moder.), #%d (%s, %s)\" % (id, pars[id][\"type\"], pars[id][\"lut\"]), x_axis_type='datetime',\n",
    "               y_axis_type=\"linear\", plot_height = 200, plot_width = plot_width,\n",
    "               tools = TOOLS)\n",
    "    if id==1:\n",
    "        vline = Span(location=dt.datetime(2019,6,6,8,0,0), dimension='height', line_color='red', line_width=1, line_dash=[2])\n",
    "        p.renderers.extend([vline])\n",
    "        mytext = Label(x=dt.datetime(2019,6,6,8,0,0), y=1200, text='Shield rem.',\n",
    "                       text_align=\"right\", text_font_size=\"9pt\")\n",
    "        p.add_layout(mytext)\n",
    "    p.xaxis.axis_label = 'Datetime'\n",
    "    p.yaxis.axis_label = 'Counts per hour'\n",
    "    p.circle(crns[id].index, crns[id].cph1_filtered, size=1, color=\"lightgrey\", legend=\"20 mins\")\n",
    "    p.line(crns6h.index+dt.timedelta(hours=3), crns6h[id], line_color=\"black\", line_width=2, legend=\"6 hours\")\n",
    "    #p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id], line_color=\"black\", line_width=2, legend=\"6 hours\")\n",
    "    #p.line(crns24h.index+dt.timedelta(hours=12), crns24h[id], line_color=\"green\", line_dash=[2], line_width=2, legend=\"24 hours\")\n",
    "    p.x_range=Range1d(min_dtime, max_dtime)\n",
    "    miny, maxy = None, None\n",
    "    if pars[id][\"type\"]==\"CRS 2000-B\":\n",
    "        miny, maxy = 1400, 2400\n",
    "    if pars[id][\"type\"]==\"CRS 1000\":\n",
    "        miny, maxy = 500, 1000\n",
    "    if pars[id][\"type\"]==\"CRS 1000-B\":\n",
    "        miny, maxy = 900, 1500\n",
    "    if pars[id][\"type\"]==\"CRS 2000\":\n",
    "        miny, maxy = 1000, 1400\n",
    "    if miny is not None:    \n",
    "        p.y_range=Range1d(miny, maxy)\n",
    "    p.legend.location = \"center_left\"\n",
    "    p.legend.background_fill_alpha = 0.6\n",
    "    #p.legend.location = \"center_left\"\n",
    "    plts.append(p)\n",
    "\n",
    "\n",
    "grid = gridplot(plts, merge_tools=False, ncols=1)\n",
    "\n",
    "output_file(htmlfile_merge, title=\"Time series of counts per hour\")\n",
    "#save(grid)\n",
    "show(grid)\n",
    "\n",
    "# Add correct header to html\n",
    "f = open(htmlfile_merge, \"r\")\n",
    "html = f.read()\n",
    "f.close()\n",
    "html = html.strip()\n",
    "replacement = '''<html>\n",
    "<div class=\"blurb\">\n",
    "\t<h1>JFC Fendt: Processed neutron counts  </h1>\n",
    "</div><!-- /.blurb -->\n",
    "<p> Data merged from SD and telemetry...counts per hour...filtered...resampled. \n",
    "'''\n",
    "html = html.replace('<html lang=\"en\">', replacement)\n",
    "f = open(htmlfile_merge, \"w\")\n",
    "f.write(\"---\\nlayout: default\\ntitle: cosmic pages\\n---\\n\")\n",
    "f.write(html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(crns6h)#, plot_kws=dict(edgecolor=\"None\", s=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in incoming neutron flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMBD station data\n",
    "\n",
    "`nmdb.txt` contains reference (background) neutron count rates from [NMDB](http://www.nmdb.eu/nest/), for stations `KIEL2`, `JUNG`, `JUNG1`, and `DRBS` (Dourbes, Belgium). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMDB data\n",
    "nmdb = pd.read_csv(\"/media/x/cosmicsense/data/fendt/nmdb/nmdb.txt\", sep=\";\", \n",
    "                   comment=\"#\", na_values=\"   null\")\n",
    "nmdb.datetime = pd.to_datetime(nmdb.datetime)\n",
    "nmdb = nmdb.set_index(\"datetime\")\n",
    "\n",
    "ax = nmdb.plot(figsize=(14,5), grid=True, ylim=(100,500))\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"cps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot()\n",
    "nmdb.JUNG.plot(legend=\"JUNG\")\n",
    "plt.legend()\n",
    "ax2 = ax1.twinx()\n",
    "nmdb.JUNG1.plot(color=\"red\", legend=\"JUNG1\")\n",
    "leg = plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several literature references (e.g. Zreda et al. 2012, Schroen et al. 2015, Andreasen et al. 2017) suggest to correct for variations in incoming neutron fluxes based on cosmic-ray neutron monitors available through http://www.nmdb.eu/nest. The idea is to compute a simple scaling factor $f_i$ based on measure neutron intensity $f_m$ and an arbitrary reference intensity $f_{ref}$ that depends on the actual neutron monitor.\n",
    "\n",
    "\\begin{equation*}\n",
    "f_i = \\frac{I_m}{I_{ref}}\n",
    "\\end{equation*}\n",
    "\n",
    "In the dissertation thesis of Schroen (2016), a reference value $f_{ref}$ of 150 cps is suggested for the monitor on Jungfraujoch (JUNG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the time series of our CRNS counts and the NMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = (nmdb.JUNG / 150.).resample(\"6H\").mean()\n",
    "fi.name=\"fi\"\n",
    "fi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in barometric pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again based on Zreda et al. (2012), Andreasen et al. (2017) and many others, a correction factor $f_p$ is suggested in order to account for variations in barometric pressure.\n",
    "\n",
    "\\begin{equation*}\n",
    "f_p = exp\\Bigl(\\frac{p_0 - p}{L}\\Bigl)\n",
    "\\end{equation*}\n",
    "\n",
    "Quoting from [Andreasen et al. (2017)](https://dl.sciencesocieties.org/publications/vzj/pdfs/16/8/vzj2017.04.0086):\n",
    "\n",
    "> [...] $L$  is  the  mass  attenuation  length  for  high-energy  neutrons and is a function of cutoff rigidity\n",
    "> (Desilets et al., 2006), $p$ is the barometric pressure at the time of measurement, and $P_0$ is an arbitrary\n",
    "> reference pressure. Note that the units of $L$, $p$, and $p_0$ can be shielding depth (g/cm2) or pressure (Pa), \n",
    "> where $1 g/cm2 = 98.0665 Pa$. If shielding depth is used, $L$ ranges from 130 g/cm2 at high latitudes to \n",
    "> 144 g/cm2 at the equator (see Fig. 1).\n",
    "\n",
    "[Zreda et al. (2012)](https://www.hydrol-earth-syst-sci.net/16/4079/2012/hess-16-4079-2012.pdf) complement that\n",
    "\n",
    "> [... $p_0$] can be selected to be the long-term average pressure at the specific site, sea-level pressure, \n",
    "> or long-term averagepressure at a different reference site.\n",
    "\n",
    "**Question: How do we quantify $p_0$? Based on site average, or just based on standard sea level pressure (1013 mbar)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we use $p_0 = 1013.25 mbar = 101325 Pa = 1033.23 g/cmÂ²$ and $L=131.6$ for Germany (Fig. 1 in Andreasen et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_press = \"/media/x/cosmicsense/data/fendt/dwd/stundenwerte_P0_02290_akt/produkt_p0_stunde_20171208_20190610_02290.txt\"\n",
    "press = pd.read_csv(f_press, sep=\";\", na_values=-999)\n",
    "press.columns = [\"station_id\", \"datetime\", \"quality\", \"p\", \"p0\", \"eor\"]\n",
    "press.datetime = pd.to_datetime(press.datetime, format=\"%Y%m%d%H\")\n",
    "press = press.set_index(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(press.p0, cs.core.corrfact_baro(np.array(press.p0), p_0, L))\n",
    "plt.xlabel(\"Pressure (mbar)\")\n",
    "plt.ylabel(\"Inverse correction factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0 = press.p0.mean()\n",
    "L = 131.6 # g/cm2\n",
    "fp = cs.core.corrfact_baro(np.array(press.p0), p_0, L)\n",
    "fp = pd.Series(fp, index=press.index)\n",
    "fp = fp.resample(\"6H\").mean()\n",
    "fp.name=\"fp\"\n",
    "fp[\"2019-05-01\":].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in atmospheric water vapor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their overview, Andreasen et al. (2017) refer to Rosolem et al. (2013) when accounting for the effects of atmospheric water vapor:\n",
    "\n",
    "\\begin{equation*}\n",
    "f_{wv} = 1 + 0.0054 * (h - h_{ref})\n",
    "\\end{equation*}\n",
    "\n",
    "where $h$ is the absolute humidity of the air (in g/m3), and $h_{ref}$ is the absolute humidity at an arbitrary reference time.\n",
    "\n",
    "The references do not elaborate on how to obtain the absolute humidity, but given the relative humidity and air temperature, we typically obtain $h$ by combining \n",
    "\n",
    "1. Relationship between vapor pressure $e$, saturated vapor pressure $e_s$ and relative humidity $rh$ (in %)\n",
    "\n",
    "\\begin{equation*}\n",
    "e = e_s * rh / 100.\n",
    "\\end{equation*}\n",
    "\n",
    "2. August-Roche-Magnus approximation of relation betweeen $e_s$ (mbar) and air temperature $T$ (in deg C) \n",
    "\n",
    "\\begin{equation*}\n",
    "e_s(T) = 6.1094 * exp\\Bigl(\\frac{17.625*T}{T + 243.04}\\Bigl)\n",
    "\\end{equation*}\n",
    "\n",
    "3. Universal law of perfect gases (with volume $V$, mass $m$, specific gas constant $R_S=461.4 J/kg/K$ for water vapor)\n",
    "\n",
    "\\begin{equation*}\n",
    "e * V = m * R_s * T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy correction factor for humidity\n",
    "fwv = pd.Series(data=np.ones(len(crns6h.index)), index=crns6h.index)\n",
    "fwv.name = \"fwv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the correction factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the different correction factor, and use them to correct our neutron counts. According to Andreasen, this is done via\n",
    "\n",
    "\\begin{equation*}\n",
    "N_{cor} = \\frac{N*f_{wv}}{f_p*f_i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hc = crns6h.copy()\n",
    "crns6hc = pd.merge(crns6hc, fi, how='left', left_index=True, right_index=True)\n",
    "crns6hc = pd.merge(crns6hc, fp, how='left', left_index=True, right_index=True)\n",
    "crns6hc = pd.merge(crns6hc, fwv, how='left', left_index=True, right_index=True)\n",
    "#crns6hc = crns6hc.rename(index=str, columns={\"JUNG1\": \"fi\", \"p0\": \"fp\"})\n",
    "#crns6hc.index = pd.to_datetime(crns6hc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crns6hc = crns6h.copy()\n",
    "for id in ids:\n",
    "    crns6hc[id] = crns6hc[id] * crns6hc[\"fwv\"] / (crns6hc[\"fi\"] * crns6hc[\"fp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prec = \"/media/x/cosmicsense/data/fendt/dwd/stundenwerte_RR_02290_akt/produkt_rr_stunde_20171208_20190610_02290.txt\"\n",
    "prec = pd.read_csv(f_prec, sep=\";\", na_values=-999)\n",
    "\n",
    "prec.columns = [\"station_id\", \"datetime\", \"quality\", \"depth\", \"ind\", \"wrtr\", \"eor\"]\n",
    "prec.datetime = pd.to_datetime(prec.datetime, format=\"%Y%m%d%H\")\n",
    "prec = prec.set_index(\"datetime\")\n",
    "prec[\"2019-05-06\":].depth.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hcst = crns6hc.copy()\n",
    "#crns6hcst = crns6hcst.drop(columns=[\"fi\", \"fp\", \"fwv\"])\n",
    "for i, id in enumerate(ids):\n",
    "    if id==19:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][id])\n",
    "        crns6hcst[id] = crns6hc[id] * fact\n",
    "    elif id==1:\n",
    "        #continue\n",
    "        fact2 = np.nanmean(crns6hc[\"2019-06-06 12:00:00\":\"2019-06-12 00:00:00\"][1]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][1])\n",
    "        crns1 = crns6hc[1].copy()\n",
    "        crns1[:\"2019-06-06 03:00:00\"] *= fact2\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns1[\"2019-05-22\":\"2019-05-29\"])\n",
    "        crns1 *= fact\n",
    "        crns6hcst[id] = crns1\n",
    "    else:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][4]) / np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][id])\n",
    "        crns6hcst[id] = crns6hc[id] * fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hcst.index+dt.timedelta(hours=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size'   : 12})\n",
    "colors = plt.cm.tab20(np.linspace(0,1,len(ids)))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax1 = fig.add_subplot(311)\n",
    "crns24chst = crns6hcst.resample(\"24H\").mean()\n",
    "#pl = crns6hcst.resample(\"24H\").mean().plot(y=ids, ax=ax1, color=colors)\n",
    "#crns6hcst.plot(ax=ax1, color=colors)\n",
    "for i, id in enumerate(ids):\n",
    "    if id==1:\n",
    "        continue\n",
    "    ax1.plot(crns6hcst.index+dt.timedelta(hours=3), crns6hcst[id], color=colors[i])\n",
    "    #ax1.plot(crns24chst.index+dt.timedelta(hours=12), crns24chst[id], color=colors[i])\n",
    "ax1.legend(ncol=2)\n",
    "plt.ylim(6500, 8000)\n",
    "plt.grid()\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.plot(prec[\"2019-05-01\":].index, prec[\"2019-05-01\":].depth.cumsum())\n",
    "#prec[crns6hcst.index[0]:crns6hcst.index[-1]].depth.cumsum().plot()\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "plt.grid()\n",
    "ax3 = fig.add_subplot(313)\n",
    "ax3.plot(fi.index, fi, label=\"fi\")\n",
    "ax3.plot(fp.index, fp, label=\"fp\")\n",
    "ax3.set_xlim(ax1.get_xlim())\n",
    "#crns6hcst.fi.plot()\n",
    "#crns6hcst.fp.plot()\n",
    "ax3.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hcst[4].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(crns[4].cph1.index, crns[4].cph1, \"bo\", ms=1)\n",
    "plt.plot(crns6h.index + dt.timedelta(hours=3), crns6h[4])\n",
    "plt.grid()\n",
    "plt.ylim(6000, 9000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cm.tab20(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfile_merge = \"/media/x/cosmicsense/git/misc/fendt/merged_crns.html\"\n",
    "TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "colors=bokeh.palettes.Category20_20#bokeh.palettes.Set3_12 + bokeh.palettes.Set2_6\n",
    "\n",
    "plts = []\n",
    "\n",
    "from bokeh.models import Span\n",
    "from bokeh.models import Label\n",
    "\n",
    "for i, id in enumerate(ids):\n",
    "    p = figure(title=\"Counts/hour (moder.), #%d (%s, %s)\" % (id, pars[id][\"type\"], pars[id][\"lut\"]), x_axis_type='datetime',\n",
    "               y_axis_type=\"linear\", plot_height = 200, plot_width = plot_width,\n",
    "               tools = TOOLS)\n",
    "    if id==1:\n",
    "        vline = Span(location=dt.datetime(2019,6,6,8,0,0), dimension='height', line_color='red', line_width=1, line_dash=[2])\n",
    "        p.renderers.extend([vline])\n",
    "        mytext = Label(x=dt.datetime(2019,6,6,8,0,0), y=1200, text='Shield rem.',\n",
    "                       text_align=\"right\", text_font_size=\"9pt\")\n",
    "        p.add_layout(mytext)\n",
    "    p.xaxis.axis_label = 'Datetime'\n",
    "    p.yaxis.axis_label = 'Counts per hour'\n",
    "    #p.circle(crns[id].index, crns[id].cph1_filtered, size=1, color=\"lightgrey\", legend=\"20 mins\")\n",
    "    p.line(crns6h.index+dt.timedelta(hours=3), crns6h[id], line_color=\"grey\", line_width=2, legend=\"uncorrected\")\n",
    "    p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id], line_color=\"black\", line_width=2, legend=\"corrected\")\n",
    "    p.line(prec[\"2019-05-06\":].index, prec[\"2019-05-06\":].depth.cumsum(), line_color=\"blue\", line_width=2, legend=\"precipitation\")\n",
    "    #p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id], line_color=\"black\", line_width=2, legend=\"6 hours\")\n",
    "    #p.line(crns24h.index+dt.timedelta(hours=12), crns24h[id], line_color=\"green\", line_dash=[2], line_width=2, legend=\"24 hours\")\n",
    "    p.x_range=Range1d(min_dtime, max_dtime)\n",
    "    miny, maxy = None, None\n",
    "    if pars[id][\"type\"]==\"CRS 2000-B\":\n",
    "        miny, maxy = 1400, 2400\n",
    "    if pars[id][\"type\"]==\"CRS 1000\":\n",
    "        miny, maxy = 500, 1000\n",
    "    if pars[id][\"type\"]==\"CRS 1000-B\":\n",
    "        miny, maxy = 900, 1500\n",
    "    if pars[id][\"type\"]==\"CRS 2000\":\n",
    "        miny, maxy = 1000, 1400\n",
    "    if miny is not None:    \n",
    "        p.y_range=Range1d(miny, maxy)\n",
    "    p.legend.location = \"center_left\"\n",
    "    p.legend.background_fill_alpha = 0.6\n",
    "    #p.legend.location = \"center_left\"\n",
    "    plts.append(p)\n",
    "\n",
    "\n",
    "grid = gridplot(plts, merge_tools=False, ncols=1)\n",
    "\n",
    "output_file(htmlfile_merge, title=\"Time series of counts per hour\")\n",
    "#save(grid)\n",
    "show(grid)\n",
    "\n",
    "# Add correct header to html\n",
    "f = open(htmlfile_merge, \"r\")\n",
    "html = f.read()\n",
    "f.close()\n",
    "html = html.strip()\n",
    "replacement = '''<html>\n",
    "<div class=\"blurb\">\n",
    "\t<h1>JFC Fendt: Processed neutron counts  </h1>\n",
    "</div><!-- /.blurb -->\n",
    "<p> Data merged from SD and telemetry...counts per hour...filtered...resampled. \n",
    "'''\n",
    "html = html.replace('<html lang=\"en\">', replacement)\n",
    "f = open(htmlfile_merge, \"w\")\n",
    "f.write(\"---\\nlayout: default\\ntitle: cosmic pages\\n---\\n\")\n",
    "f.write(html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfile_merge = \"/media/x/cosmicsense/git/misc/fendt/merged_crns.html\"\n",
    "TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "colors=bokeh.palettes.Category20_20#bokeh.palettes.Set3_12 + bokeh.palettes.Set2_6\n",
    "\n",
    "plts = []\n",
    "\n",
    "from bokeh.models import Span\n",
    "from bokeh.models import Label\n",
    "\n",
    "p = figure(title=\"Counts/hour (moder.), norm. to CRNS #4; cum. prec. (H.-Peiss.)\", x_axis_type='datetime',\n",
    "           y_axis_type=\"linear\", plot_height = 800, plot_width = 800,#plot_width,\n",
    "           tools = TOOLS)\n",
    "\n",
    "for i, id in enumerate(ids):\n",
    "    if id==19:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][id])\n",
    "        p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id]*fact, line_color=colors[i], line_width=1, legend=str(id))\n",
    "    elif id==1:\n",
    "        #continue\n",
    "        fact2 = np.nanmean(crns6hc[\"2019-06-06 12:00:00\":\"2019-06-12 00:00:00\"][1]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][1])\n",
    "        crns1 = crns6hc[1].copy()\n",
    "        crns1[:\"2019-06-06 03:00:00\"] *= fact2\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns1[\"2019-05-22\":\"2019-05-29\"])\n",
    "        crns1 *= fact\n",
    "        p.line(crns6hc.index+dt.timedelta(hours=3), crns1, line_color=colors[i], line_width=1, legend=str(id))\n",
    "    else:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][4]) / np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][id])\n",
    "        p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id]*fact, line_color=colors[i], line_width=1, legend=str(id))\n",
    "    print(fact)\n",
    "    if id==1:\n",
    "        vline = Span(location=dt.datetime(2019,6,6,8,0,0), dimension='height', line_color='red', line_width=1, line_dash=[2])\n",
    "        p.renderers.extend([vline])\n",
    "        mytext = Label(x=dt.datetime(2019,6,6,8,0,0), y=1200, text='Shield rem.',\n",
    "                       text_align=\"right\", text_font_size=\"9pt\")\n",
    "        p.add_layout(mytext)\n",
    "    p.xaxis.axis_label = 'Datetime'\n",
    "    p.yaxis.axis_label = 'Counts per hour'\n",
    "    #p.circle(crns[id].index, crns[id].cph1_filtered, size=1, color=\"lightgrey\", legend=\"20 mins\")\n",
    "    #p.line(crns6h.index+dt.timedelta(hours=3), crns6h[id], line_color=\"grey\", line_width=2, legend=\"uncorrected\")\n",
    "    #p.line(prec[\"2019-05-06\":].index, prec[\"2019-05-06\":].depth.cumsum(), line_color=\"blue\", line_width=2, legend=\"precipitation\")\n",
    "    #p.line(crns6hc.index+dt.timedelta(hours=3), crns6hc[id], line_color=\"black\", line_width=2, legend=\"6 hours\")\n",
    "    #p.line(crns24h.index+dt.timedelta(hours=12), crns24h[id], line_color=\"green\", line_dash=[2], line_width=2, legend=\"24 hours\")\n",
    "    p.y_range=Range1d(5000, 9000)\n",
    "    p.x_range=Range1d(min_dtime, max_dtime)\n",
    "    p.legend.location = \"center_left\"\n",
    "    p.legend.background_fill_alpha = 0.6\n",
    "    #p.legend.location = \"center_left\"\n",
    "    plts.append(p)\n",
    "\n",
    "p.extra_y_ranges = {\n",
    "    \"Precipitation\": Range1d(\n",
    "        0, 250\n",
    "    )\n",
    "}\n",
    "p.add_layout(LinearAxis(y_range_name=\"Precipitation\"), \"right\")\n",
    "\n",
    "p.line(prec[\"2019-05-06\":].index, prec[\"2019-05-06\":].depth.cumsum(), y_range_name=\"Precipitation\", \n",
    "       line_color=\"black\", line_width=2, line_dash = [2], legend=\"Prec.)\")\n",
    "output_file(htmlfile_merge, title=\"Time series of counts per hour\")\n",
    "#save(grid)\n",
    "show(p)\n",
    "\n",
    "# Add correct header to html\n",
    "f = open(htmlfile_merge, \"r\")\n",
    "html = f.read()\n",
    "f.close()\n",
    "html = html.strip()\n",
    "replacement = '''<html>\n",
    "<div class=\"blurb\">\n",
    "\t<h1>JFC Fendt: Processed neutron counts  </h1>\n",
    "</div><!-- /.blurb -->\n",
    "<p> Data merged from SD and telemetry...counts per hour...filtered...resampled. \n",
    "'''\n",
    "html = html.replace('<html lang=\"en\">', replacement)\n",
    "f = open(htmlfile_merge, \"w\")\n",
    "f.write(\"---\\nlayout: default\\ntitle: cosmic pages\\n---\\n\")\n",
    "f.write(html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hc[\"2019-05-15\":\"2019-06-22\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From corrected neutron counts to soil moisture estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volumetric soil moisture is estimated from a single relation that includes a local calibration parameter $N_0$. [Desilets et al. (2010)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2009WR008726) suggested the following one:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = \\Bigl(\\frac{a_0}{\\frac{N_{corr}}{N_0} - a_1} - a_2\\Bigl) * \\frac{\\rho_b}{\\rho_w}\n",
    "\\end{equation*}\n",
    "\n",
    "While $a_0$, $a_1$ and $a_2$ are empirical parameters, soil bulk density $\\rho_b$ can be measured or retrieved via transfer functions or literature, the density of water $\\rho_w$ is a function of temperature and salinity, although the approximate value of 1000 kg/m3 is can be used without introducing much error as compared to other uncertainties.\n",
    "\n",
    "The parameter $N_0$ depends on the local hydrogen pool and footprint, and needs to be obtained from calibration with gravimetric soil moisture measurements in the footprint at a defined point in time. For the time being, we just take a wild guess and set $N_0$ to 1500 cph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cs.core.n_to_theta_desilets(crns6hc, n0=1500.).plot(figsize=(14,12), grid=True, subplots=True)\n",
    "for ax_ in ax:\n",
    "    ax_.set_ylabel(\"theta (m3/m3)\")\n",
    "    ax_.set_ylim(-0.1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local calibration of $N_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gravimetric soil moisture measurements are organised in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(\"../../../data/crns_sample1_soil.txt\", sep=\";\")\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_cyl = 98. # cm3\n",
    "samples[\"theta\"] = (samples.wet_cyl_lab_g - samples.dry_cyl_lab_g) / vol_cyl\n",
    "samples[\"rho_b\"] = (samples.dry_cyl_lab_g - samples.cyl_g) / vol_cyl\n",
    "samples.depth = [depth.strip() for depth in samples.depth]\n",
    "samples[\"depth2\"] = np.array([float(depth.split(\"-\")[0]) for depth in samples.depth])\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling design is described in [Rivera Villarreyes, 2013](https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/6730/file/rivera_villarreyes_diss.pdf), p. 104. Letter `A-F` refer to the six angles, numbers `1-4` to the distances `25, 75, 175, and 225` respectively. We assign the distannce of the measurements to the CRNS probe on that basis, assuming that \"Center\" means a distance of 1 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"r\"] = np.nan\n",
    "for i in range(len(samples)):\n",
    "    if \"1\" in samples.location[i]:\n",
    "        samples.loc[i, \"r\"] = 25.\n",
    "    elif \"2\" in samples.location[i]:\n",
    "        samples.loc[i, \"r\"] = 75.\n",
    "    elif \"3\" in samples.location[i]:\n",
    "        samples.loc[i, \"r\"] = 175.\n",
    "    elif \"4\" in samples.location[i]:\n",
    "        samples.loc[i, \"r\"] = 225.\n",
    "    elif samples.location[i]==\"Center\":\n",
    "        samples.loc[i, \"r\"] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "ax = plt.subplot(111)\n",
    "for loc in np.unique(samples.location):\n",
    "    depths = samples[samples.location==loc].depth2\n",
    "    theta = samples[samples.location==loc].theta\n",
    "    #print(theta, depths)\n",
    "    plt.plot(theta, -depths-2.5, label=\"_exclude\")\n",
    "avg = samples.groupby(\"depth2\").mean()\n",
    "plt.plot(avg.theta, -avg.index-2.5, linewidth=4, color=\"black\", label=\"Mean profile\")\n",
    "plt.xlabel(\"Theta (m3/m3)\")\n",
    "plt.ylabel(\"Depth (cm)\")\n",
    "plt.title(\"Measured theta profiles and average profile\")\n",
    "plt.grid()\n",
    "leg = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schroen et al. (2017) suggest a five step procedure for obtaining the average soil moisture `theta_avg` in the CRNS footprint:\n",
    "\n",
    "1. **Initial guess:** obtain initial estimate of `theta_avg` (e.g. as the simple mean of all measurements)\n",
    "2. **Penetration depth:** compute the penetration depth for each profile, depending on distance `r`, `theta_avg`, as well as barometric pressure `press`, vegetation height `Hveg`, and bulk density `rho_b`.\n",
    "3. **Vertical averaging:** compute the average soil moisture for each measured profile, based on vertical weights for each profile (implicitely requires penetration depth).\n",
    "4. **Horizontal averaging:** compute `theta_avg` by as a weighted average, obtaining horizontal weights as a function of distance `r`, `theta_avg`, as well as barometric pressure `press`, vegetation height `Hveg`, and absolute humidity `h`.\n",
    "5. Iterate over 1-4 until `theta_avg` converges to a user-defined accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, the approach to first apply vertical averaging and *then* horizontal averaging only makes sense if the sampling is identical for each profile. E.g., if we had six samples per profile, but one profil with only *one* sample, that last profile would be overemphasized if we would first average on a per-profile basis. It would thus be more adequate to apply vertical and horizontal weighting simultaneously. Beyond, *step 2* of the above procedure could be considered as part of *step 3*, since the penetration depth is required to obtain the vertical weights.\n",
    "\n",
    "On the basis, we suggest the following revised procedure:\n",
    "\n",
    "1. Initial guess\n",
    "2. Vertical and horizontal averaging\n",
    "3. Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: initial guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial guess of $\\theta$ is the average of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_avg = samples.theta.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: penetration depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters which are asssumed to be uniform across the footprint\n",
    "rho_b = samples.rho_b.mean()\n",
    "press = crns[\"tower\"][\"2013-08-05\"].press.mean()\n",
    "h = np.mean(cs.conv.absolute_humidity(crns[\"tower\"][\"2013-08-05\"].temp1, crns[\"tower\"][\"2013-08-05\"].relhum1) )\n",
    "Hveg = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_p = cs.core.D86(samples.r, theta=theta_avg, press=press, Hveg=Hveg, rhob=rho_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: vertical averaging per profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs = pd.DataFrame({}, index=np.unique(samples.location) )\n",
    "profs[\"theta\"] = np.nan\n",
    "profs[\"r\"] = np.nan\n",
    "for prof in profs.index:\n",
    "    ix = samples.location==prof\n",
    "    profs.loc[prof, \"r\"] = np.unique(samples[ix].r)[0]\n",
    "    vweights = cs.core.vertical_weight_koehli(profs.loc[prof, \"r\"], samples[ix].depth2, theta_avg, press, Hveg, rho_b)\n",
    "    profs.loc[prof, \"theta\"] = np.sum(vweights * samples[ix].theta) / np.sum(vweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: horizontal averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hweights = cs.core.horizontal_weight_koehli(profs.r, press, Hveg, theta_avg, h)\n",
    "theta_avg = np.sum(hweights * profs.theta) / np.sum(hweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: iterate until `theta_avg` converges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5\n",
    "theta_avg = samples.theta.mean()\n",
    "for i in range(max_iter):\n",
    "    print(i, np.round(theta_avg,3))\n",
    "    # step 2\n",
    "    D_p = cs.core.D86(samples.r, theta=theta_avg, press=press, Hveg=Hveg, rhob=rho_b)\n",
    "    # step 3\n",
    "    profs = pd.DataFrame({}, index=np.unique(samples.location) )\n",
    "    profs[\"theta\"] = np.nan\n",
    "    profs[\"r\"] = np.nan\n",
    "    for prof in profs.index:\n",
    "        ix = samples.location==prof\n",
    "        profs.loc[prof, \"r\"] = np.unique(samples[ix].r)[0]\n",
    "        vweights = cs.core.vertical_weight_koehli(profs.loc[prof, \"r\"], samples[ix].depth2, theta_avg, press, Hveg, rho_b)\n",
    "        profs.loc[prof, \"theta\"] = np.sum(vweights * samples[ix].theta) / np.sum(vweights)\n",
    "    # step 4\n",
    "    hweights = cs.core.horizontal_weight_koehli(profs.r, press, Hveg, theta_avg, h)\n",
    "    theta_avg = np.sum(hweights * profs.theta) / np.sum(hweights)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only one single set of soil moisture measurements for a single point in time, calibration only involves solving the relationship between $\\theta (N)$ for $N_0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "N_0 = N * \\frac{\\theta * \\frac{\\rho_w}{\\rho_b} + a_2}{a_0 + \\theta * \\frac{\\rho_w}{\\rho_b} * a_1 + a_1 * a_2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_n0_desilets(theta, cph, a0=0.0808, a1=0.372, a2=0.115, rhob=1500., rhow=1000.):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return cph * (theta*(rhow/rhob) + a2) / (a0 + theta*(rhow/rhob) * a1 + a1 * a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = crns6hc[\"2013-08-05 00:00:00\":\"2013-08-06 00:00:00\"].tower.mean()\n",
    "n0 = calibrate_n0_desilets(theta_avg, cph)\n",
    "print(\"N0 =\", n0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = cs.core.n_to_theta_desilets(crns6hc.resample(\"168H\").mean(), n0=n0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = theta.tower.plot(figsize=(14,12), grid=True, subplots=True)\n",
    "for ax_ in ax:\n",
    "    ax_.set_ylabel(\"theta (m3/m3)\")\n",
    "    ax_.set_ylim(-0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
