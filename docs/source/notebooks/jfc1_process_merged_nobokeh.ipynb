{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JFC #1 Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import cosmicsense as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('once', RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# Display figures inline\n",
    "%matplotlib\n",
    "# Display figures as interactive (requires kernel restart)\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation at Hohenpeißenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maik/miniconda3/envs/cosmic-sense/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "f_prec = \"/media/x/cosmicsense/data/fendt/dwd/stundenwerte_RR_02290_akt/produkt_rr_stunde_20171208_20190610_02290.txt\"\n",
    "prec = pd.read_csv(f_prec, sep=\";\", na_values=-999)\n",
    "\n",
    "prec.columns = [\"station_id\", \"datetime\", \"quality\", \"depth\", \"ind\", \"wrtr\", \"eor\"]\n",
    "prec.datetime = pd.to_datetime(prec.datetime, format=\"%Y%m%d%H\")\n",
    "prec = prec.set_index(\"datetime\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(prec[\"2019-05-06\":].index, prec[\"2019-05-06\":].depth.cumsum())\n",
    "#prec[\"2019-05-06\":].depth.cumsum().plot()\n",
    "plt.grid() \n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRNS records from three probes with different temporal coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [1, 2, 3, 4, 5, 6, 7, 8, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25]\n",
    "\n",
    "#fpath = \"/home/maik/b2drop/cosmicsense/inbox/fendt/timeseries/crns/JFC-1/\"\n",
    "fpath = \"/media/x/cosmicsense/data/fendt/crns/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 2019-05-09 09:59:00 to 2019-06-13 13:19:00\n",
      "2: 2019-05-07 11:02:13 to 2019-06-13 11:40:00\n",
      "3: 2019-05-07 08:37:25 to 2019-06-13 09:26:00\n",
      "4: 2019-05-07 15:21:29 to 2019-06-13 11:39:00\n",
      "5: 2019-05-03 08:53:04 to 2019-06-07 08:37:00\n",
      "6: 2019-05-03 09:34:48 to 2019-06-07 08:41:00\n",
      "7: 2019-05-13 14:54:00 to 2019-06-07 09:40:00\n",
      "8: 2019-05-01 00:46:00 to 2019-06-07 08:46:00\n",
      "14: 2019-05-08 06:45:44 to 2019-05-31 09:19:00\n",
      "16: 2019-05-14 13:45:00 to 2019-06-12 16:31:00\n",
      "17: 2019-05-15 14:14:46 to 2019-06-12 14:26:00\n",
      "18: 2019-05-14 10:16:06 to 2019-06-06 14:35:00\n",
      "19: 2019-05-14 12:45:49 to 2019-06-13 09:00:00\n",
      "21: 2019-05-13 13:38:38 to 2019-06-10 20:56:00\n",
      "22: 2019-05-13 15:24:44 to 2019-06-10 21:18:00\n",
      "23: 2019-05-15 15:55:32 to 2019-06-13 10:19:00\n",
      "24: 2019-05-15 15:09:58 to 2019-06-12 15:19:00\n",
      "25: 2019-05-14 10:02:00 to 2019-06-07 08:41:00\n"
     ]
    }
   ],
   "source": [
    "crns = {}\n",
    "for id in ids:\n",
    "    df = pd.read_csv(path.join(fpath, \"%d/%d_CRNS_merge.txt\" % (id, id)), sep=\"\\t\")\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    df = df.set_index(\"datetime\")\n",
    "    if id==4:\n",
    "        df[\"cph1\"] = (df.counts1 + df.counts2) / cs.conv.s_to_h(df.nsecs1)\n",
    "    else:\n",
    "        df[\"cph1\"] = df.counts1 / cs.conv.s_to_h(df.nsecs1)\n",
    "        try:\n",
    "            df[\"cph2\"] = df.counts2 / cs.conv.s_to_h(df.nsecs2)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    print(id, end=\": \")\n",
    "    print(\"%s to %s\" % (df.index[0], df.index[-1]) )\n",
    "    crns[id] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dtime = np.min([crns[key].index[0] for key in crns.keys()])\n",
    "max_dtime = np.max([crns[key].index[-1] for key in crns.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter spurious signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some probes are affected by spurious count rates. In order to keep useful parts of the signal, a pragmatic/heuristic filtering approach is applied which could be refined later:\n",
    "\n",
    "1. Remove entirely unrealisticly count rates for specific probes (`mincph`, `maxcph`)\n",
    "2. Remove count rates from spuriously short count intervals (`mininterv`)\n",
    "3. After that, there are still spurious count rates.In order to detect these, we compute the maximum count rates over periods of six hours, and then apply a 24-hour-median filter to these 6-hour-maxima. That way, we try to truncate spurios peaks. In order to prevent too aggressive filtering, we add a `buffer` of the median count rates over a period of 24 hours.\n",
    "4. We then interpolate this upper limit filter to the original timestamp values and use it to remove high values.\n",
    "5. We remove unrealisticly low count rates (`mincph`), and than apply the same approach (points 3-4) to eliminate spuriously small values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars =  {\n",
    "    1: {\"mincph\": 500, \"maxcph\": 1000, \"mininterv\": -9999, \"type\": \"CRS 2000-B, Scn.\", \"lut\": \"forest/meadow\"},\n",
    "    2: {\"mincph\": 500, \"maxcph\": 1100, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    3: {\"mincph\": 500, \"maxcph\": 1100, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    4: {\"mincph\": 6000, \"maxcph\": 9500, \"mininterv\": -9999, \"type\": \"Lab C\", \"lut\": \"meadow\"},\n",
    "    5: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    6: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow, forest close\"},\n",
    "    7: {\"mincph\": 1000, \"maxcph\": 1700, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    8: {\"mincph\": 1300, \"maxcph\": 2500, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    14: {\"mincph\": 800, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 2000\", \"lut\": \"forest\"},\n",
    "    16: {\"mincph\": 1500, \"maxcph\": 2500, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    17: {\"mincph\": 1400, \"maxcph\": 2400, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    18: {\"mincph\": 500, \"maxcph\": 1000, \"mininterv\": -9999, \"type\": \"CRS 1000\", \"lut\": \"meadow\"},\n",
    "    19: {\"mincph\": 1300, \"maxcph\": 2300, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"forest\"},\n",
    "    21: {\"mincph\": 1400, \"maxcph\": 2300, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow, forest close\"},\n",
    "    22: {\"mincph\": 1100, \"maxcph\": 2100, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"forest\"},\n",
    "    23: {\"mincph\": 1200, \"maxcph\": 2200, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow, peat\"},\n",
    "    24: {\"mincph\": 1600, \"maxcph\": 2600, \"mininterv\": -9999, \"type\": \"CRS 2000-B\", \"lut\": \"meadow\"},\n",
    "    25: {\"mincph\": 900, \"maxcph\": 1500, \"mininterv\": -9999, \"type\": \"CRS 1000-B\", \"lut\": \"meadow\"},\n",
    "    \n",
    "}\n",
    "\n",
    "buffer = 0.075\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    x = crns[key].cph1.copy()\n",
    "    if not key==1:\n",
    "        x[x > pars[key][\"maxcph\"]] = np.nan\n",
    "    x[x < pars[key][\"mincph\"]] = np.nan\n",
    "    x[crns[key].nsecs1 < pars[key][\"mininterv\"]] = np.nan\n",
    "    median24 = x.resample(\"24H\").median()\n",
    "    # Maxfilter\n",
    "    max6 = x.resample(\"6H\").max()\n",
    "    median24max6 = max6.resample(\"24H\").median()\n",
    "    maxfilter = np.array(median24max6 + buffer * median24)\n",
    "    # Minfilter\n",
    "    min6 = x.resample(\"6H\").min()\n",
    "    median24min6 = min6.resample(\"24H\").median()\n",
    "    minfilter = np.array(median24min6 - buffer * median24)    \n",
    "    # Resample filter to original time stamps\n",
    "    crns[key][\"cph1_maxfilter\"] = np.interp(x.index, median24.index, maxfilter)\n",
    "    crns[key][\"cph1_minfilter\"] = np.interp(x.index, median24.index, minfilter)\n",
    "    # Fill gaps\n",
    "    crns[key][\"cph1_maxfilter\"] = crns[key].cph1_maxfilter.interpolate()\n",
    "    crns[key][\"cph1_minfilter\"] = crns[key].cph1_minfilter.interpolate()\n",
    "    # Apply filter\n",
    "    crns[key][\"cph1_filtered\"] = x\n",
    "    if not key==1:\n",
    "        crns[key].loc[crns[key].cph1 > crns[key].cph1_maxfilter, \"cph1_filtered\"] = np.nan\n",
    "        crns[key].loc[crns[key].cph1 < crns[key].cph1_minfilter, \"cph1_filtered\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size'   : 12})\n",
    "fig, ax = plt.subplots(nrows=len(crns), figsize=(12,40))\n",
    "\n",
    "xlim = min_dtime, max_dtime\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1, linestyle=\"None\", marker=\".\", ms=1, color=\"red\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_maxfilter, linestyle=\"-\", ms=0, color=\"orange\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_minfilter, linestyle=\"-\", ms=0, color=\"orange\")\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_filtered, linestyle=\"None\", marker=\".\", ms=1, color=\"black\")\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "    ax[i].set_ylabel(\"cph\")\n",
    "    ax[i].set_xlim(xlim)\n",
    "    #ax[i].set_ylim(pars[key][\"mincph\"], pars[key][\"maxcph\"]+200)\n",
    "    ax[i].grid()\n",
    "    #ax[i].legend()    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to a common time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-05-01 00:46:00'), Timestamp('2019-06-13 13:19:00'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_dtime, max_dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrange6 = pd.date_range('2019-05-01 00:00:00', max_dtime, freq=\"6H\")\n",
    "dtrange24 = pd.date_range('2019-05-01 00:00:00', max_dtime, freq=\"24H\")\n",
    "crns6h = pd.DataFrame({}, index=dtrange6)\n",
    "crns24h = pd.DataFrame({}, index=dtrange24)\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    crns6h = pd.merge(crns6h, crns[key].cph1_filtered.resample(\"6H\").mean(), \n",
    "                      how=\"left\", left_index=True, right_index=True)\n",
    "    crns6h[key] = crns6h.cph1_filtered\n",
    "    crns6h = crns6h.drop(\"cph1_filtered\", axis=1)\n",
    "    # 24 h\n",
    "    crns24h = pd.merge(crns24h, crns[key].cph1_filtered.resample(\"24H\").mean(), \n",
    "                      how=\"left\", left_index=True, right_index=True)\n",
    "    crns24h[key] = crns24h.cph1_filtered\n",
    "    crns24h = crns24h.drop(\"cph1_filtered\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size'   : 12})\n",
    "fig, ax = plt.subplots(nrows=len(crns), figsize=(12,40))\n",
    "\n",
    "xlim = min_dtime, max_dtime\n",
    "\n",
    "for i, key in enumerate(crns.keys()):\n",
    "    ax[i].plot(crns[key].index, crns[key].cph1_filtered, linestyle=\"None\", marker=\".\", ms=1, color=\"grey\")\n",
    "    ax[i].plot(crns6h.index + dt.timedelta(hours=3), crns6h[key], \"k-\")#linestyle=\"None\", marker=\"None\", ms=1, color=\"black\")\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "    ax[i].set_ylabel(\"cph\")\n",
    "    ax[i].set_xlim(xlim)\n",
    "    ax[i].grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maik/miniconda3/envs/cosmic-sense/lib/python3.7/site-packages/numpy/lib/histograms.py:754: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "/home/maik/miniconda3/envs/cosmic-sense/lib/python3.7/site-packages/numpy/lib/histograms.py:755: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    }
   ],
   "source": [
    "pp = sb.pairplot(crns6h)#, plot_kws=dict(edgecolor=\"None\", s=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in incoming neutron flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMBD station data\n",
    "\n",
    "`nmdb.txt` contains reference (background) neutron count rates from [NMDB](http://www.nmdb.eu/nest/), for stations `KIEL2`, `JUNG`, `JUNG1`, and `DRBS` (Dourbes, Belgium). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMDB data\n",
    "nmdb = pd.read_csv(\"/media/x/cosmicsense/data/fendt/nmdb/nmdb.txt\", sep=\";\", \n",
    "                   comment=\"#\", na_values=\"   null\")\n",
    "nmdb.datetime = pd.to_datetime(nmdb.datetime)\n",
    "nmdb = nmdb.set_index(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f53a1eb9f98>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmdb.plot(ylim=(0,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Incoming neutron flux at Jungfraujoch')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "plt.plot(nmdb.index, nmdb.JUNG, \"b-\", label=\"JUNG\")\n",
    "plt.ylabel(\"counts / s\")\n",
    "plt.legend()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "plt.plot(nmdb.index, nmdb.JUNG1, \"r-\", label=\"JUNG1\")\n",
    "plt.ylabel(\"counts / s\")\n",
    "leg = plt.legend(loc=\"lower right\")\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "plt.title(\"Incoming neutron flux at Jungfraujoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several literature references (e.g. Zreda et al. 2012, Schroen et al. 2015, Andreasen et al. 2017) suggest to correct for variations in incoming neutron fluxes based on cosmic-ray neutron monitors available through http://www.nmdb.eu/nest. The idea is to compute a simple scaling factor $f_i$ based on measure neutron intensity $f_m$ and an arbitrary reference intensity $f_{ref}$ that depends on the actual neutron monitor.\n",
    "\n",
    "\\begin{equation*}\n",
    "f_i = \\frac{I_m}{I_{ref}}\n",
    "\\end{equation*}\n",
    "\n",
    "In the dissertation thesis of Schroen (2016), a reference value $f_{ref}$ of 150 cps is suggested for the monitor on Jungfraujoch (JUNG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737179.9902777778, 737223.575)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = (nmdb.JUNG / 150.).resample(\"6H\").mean()\n",
    "fi.name=\"fi\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plt.plot(fi.index, fi)\n",
    "plt.grid()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.title(\"Correction factor for incoming neutron flux (JUNG)\")\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in barometric pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again based on Zreda et al. (2012), Andreasen et al. (2017) and many others, a correction factor $f_p$ is suggested in order to account for variations in barometric pressure.\n",
    "\n",
    "\\begin{equation*}\n",
    "f_p = exp\\Bigl(\\frac{p_0 - p}{L}\\Bigl)\n",
    "\\end{equation*}\n",
    "\n",
    "Quoting from [Andreasen et al. (2017)](https://dl.sciencesocieties.org/publications/vzj/pdfs/16/8/vzj2017.04.0086):\n",
    "\n",
    "> [...] $L$  is  the  mass  attenuation  length  for  high-energy  neutrons and is a function of cutoff rigidity\n",
    "> (Desilets et al., 2006), $p$ is the barometric pressure at the time of measurement, and $P_0$ is an arbitrary\n",
    "> reference pressure. Note that the units of $L$, $p$, and $p_0$ can be shielding depth (g/cm2) or pressure (Pa), \n",
    "> where $1 g/cm2 = 98.0665 Pa$. If shielding depth is used, $L$ ranges from 130 g/cm2 at high latitudes to \n",
    "> 144 g/cm2 at the equator (see Fig. 1).\n",
    "\n",
    "[Zreda et al. (2012)](https://www.hydrol-earth-syst-sci.net/16/4079/2012/hess-16-4079-2012.pdf) complement that\n",
    "\n",
    "> [... $p_0$] can be selected to be the long-term average pressure at the specific site, sea-level pressure, \n",
    "> or long-term averagepressure at a different reference site.\n",
    "\n",
    "How do we quantify $p_0$? Based on site average, or just based on standard sea level pressure (1013 mbar)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we use $p_0 = 1013.25 mbar = 101325 Pa = 1033.23 g/cm²$ or site average, and $L=131.6$ for Germany (Fig. 1 in Andreasen et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_press = \"/media/x/cosmicsense/data/fendt/dwd/stundenwerte_P0_02290_akt/produkt_p0_stunde_20171208_20190610_02290.txt\"\n",
    "press = pd.read_csv(f_press, sep=\";\", na_values=-999)\n",
    "press.columns = [\"station_id\", \"datetime\", \"quality\", \"p\", \"p0\", \"eor\"]\n",
    "press.datetime = pd.to_datetime(press.datetime, format=\"%Y%m%d%H\")\n",
    "press = press.set_index(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0 = press.p0.mean()\n",
    "L = 131.6 # g/cm2\n",
    "fp = cs.core.corrfact_baro(press.p0, p_0, L)\n",
    "\n",
    "fp = fp.resample(\"6H\").mean()\n",
    "fp.name=\"fp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Correction factor for barometric pressure')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plt.plot(fp[\"2019-05-01\":].index, fp[\"2019-05-01\":])\n",
    "plt.grid()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "plt.title(\"Correction factor for barometric pressure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for variations in atmospheric water vapor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their overview, Andreasen et al. (2017) refer to Rosolem et al. (2013) when accounting for the effects of atmospheric water vapor:\n",
    "\n",
    "\\begin{equation*}\n",
    "f_{wv} = 1 + 0.0054 * (h - h_{ref})\n",
    "\\end{equation*}\n",
    "\n",
    "where $h$ is the absolute humidity of the air (in g/m3), and $h_{ref}$ is the absolute humidity at an arbitrary reference time.\n",
    "\n",
    "The references do not elaborate on how to obtain the absolute humidity, but given the relative humidity and air temperature, we typically obtain $h$ by combining \n",
    "\n",
    "1. Relationship between vapor pressure $e$, saturated vapor pressure $e_s$ and relative humidity $rh$ (in %)\n",
    "\n",
    "\\begin{equation*}\n",
    "e = e_s * rh / 100.\n",
    "\\end{equation*}\n",
    "\n",
    "2. August-Roche-Magnus approximation of relation betweeen $e_s$ (mbar) and air temperature $T$ (in deg C) \n",
    "\n",
    "\\begin{equation*}\n",
    "e_s(T) = 6.1094 * exp\\Bigl(\\frac{17.625*T}{T + 243.04}\\Bigl)\n",
    "\\end{equation*}\n",
    "\n",
    "3. Universal law of perfect gases (with volume $V$, mass $m$, specific gas constant $R_S=461.4 J/kg/K$ for water vapor)\n",
    "\n",
    "\\begin{equation*}\n",
    "e * V = m * R_s * T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy correction factor for humidity\n",
    "fwv = pd.Series(data=np.ones(len(crns6h.index)), index=crns6h.index)\n",
    "fwv.name = \"fwv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the correction factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the different correction factor, and use them to correct our neutron counts. According to Andreasen, this is done via\n",
    "\n",
    "\\begin{equation*}\n",
    "N_{cor} = \\frac{N*f_{wv}}{f_p*f_i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hc = crns6h.copy()\n",
    "crns6hc = pd.merge(crns6hc, fi, how='left', left_index=True, right_index=True)\n",
    "crns6hc = pd.merge(crns6hc, fp, how='left', left_index=True, right_index=True)\n",
    "crns6hc = pd.merge(crns6hc, fwv, how='left', left_index=True, right_index=True)\n",
    "for id in ids:\n",
    "    crns6hc[id] = crns6hc[id] * crns6hc[\"fwv\"] / (crns6hc[\"fi\"] * crns6hc[\"fp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize all probes to one level (adjust to probe #4 - Lab C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns6hcst = crns6hc.copy()\n",
    "#crns6hcst = crns6hcst.drop(columns=[\"fi\", \"fp\", \"fwv\"])\n",
    "for i, id in enumerate(ids):\n",
    "    if id==19:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][id])\n",
    "        crns6hcst[id] = crns6hc[id] * fact\n",
    "    elif id==1:\n",
    "        #continue\n",
    "        fact2 = np.nanmean(crns6hc[\"2019-06-06 12:00:00\":\"2019-06-12 00:00:00\"][1]) / np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][1])\n",
    "        crns1 = crns6hc[1].copy()\n",
    "        crns1[:\"2019-06-06 03:00:00\"] *= fact2\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-22\":\"2019-05-29\"][4]) / np.nanmean(crns1[\"2019-05-22\":\"2019-05-29\"])\n",
    "        crns1 *= fact\n",
    "        crns6hcst[id] = crns1\n",
    "    else:\n",
    "        fact = np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][4]) / np.nanmean(crns6hc[\"2019-05-15\":\"2019-05-22\"][id])\n",
    "        crns6hcst[id] = crns6hc[id] * fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crns24chst = crns6hcst.resample(\"24H\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size'   : 12})\n",
    "colors = plt.cm.tab20(np.linspace(0,1,len(ids)))\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax1 = fig.add_subplot(411)\n",
    "\n",
    "#pl = crns6hcst.resample(\"24H\").mean().plot(y=ids, ax=ax1, color=colors)\n",
    "#crns6hcst.plot(ax=ax1, color=colors)\n",
    "for i, id in enumerate(ids):\n",
    "    if id==1:\n",
    "        continue\n",
    "        #pass\n",
    "    ax1.plot(crns6hcst.index+dt.timedelta(hours=3), crns6hcst[id], color=colors[i])\n",
    "    #ax1.plot(crns24chst.index+dt.timedelta(hours=12), crns24chst[id], color=colors[i])\n",
    "ax1.legend(ncol=2)\n",
    "plt.ylim(6500, 8000)\n",
    "plt.grid()\n",
    "plt.title(\"Resampled cph (6 hours)\")\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "\n",
    "ax = fig.add_subplot(412)\n",
    "crns24chst = crns6hcst.resample(\"24H\").mean()\n",
    "#pl = crns6hcst.resample(\"24H\").mean().plot(y=ids, ax=ax1, color=colors)\n",
    "#crns6hcst.plot(ax=ax1, color=colors)\n",
    "for i, id in enumerate(ids):\n",
    "    if id==1:\n",
    "        continue\n",
    "        #pass\n",
    "    #ax1.plot(crns6hcst.index+dt.timedelta(hours=3), crns6hcst[id], color=colors[i])\n",
    "    ax.plot(crns24chst.index+dt.timedelta(hours=12), crns24chst[id], color=colors[i])\n",
    "#ax.legend(ncol=2)\n",
    "plt.ylim(6500, 8000)\n",
    "plt.grid()\n",
    "plt.title(\"Resampled cph (24 hours)\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "\n",
    "ax2 = fig.add_subplot(413)\n",
    "ax2.plot(prec[\"2019-05-01\":].index, prec[\"2019-05-01\":].depth.cumsum())\n",
    "#prec[crns6hcst.index[0]:crns6hcst.index[-1]].depth.cumsum().plot()\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "plt.grid()\n",
    "plt.title(\"Cumulative precipitation depth (Hohenpeißenberg)\")\n",
    "\n",
    "ax3 = fig.add_subplot(414)\n",
    "ax3.plot(fi.index, fi, label=\"fi\")\n",
    "ax3.plot(fp.index, fp, label=\"fp\")\n",
    "ax3.set_xlim(ax1.get_xlim())\n",
    "ax3.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Correction factors\")\n",
    "ax3.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "plt.xlim(min_dtime-dt.timedelta(hours=1), None)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f53a0b24630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crns6hcst[4].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 9000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(crns[4].cph1.index, crns[4].cph1, \"bo\", ms=1)\n",
    "plt.plot(crns6h.index + dt.timedelta(hours=3), crns6h[4])\n",
    "plt.grid()\n",
    "plt.ylim(6000, 9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
