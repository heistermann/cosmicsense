{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "sddir = \"/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/sd\"\n",
    "remotedir = \"/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/remote\"\n",
    "trgdir = \"/media/x/cosmicsense/data/marquardt/crns\"\n",
    "tmpfile = \"tmpfile.txt\"\n",
    "tmpfile2 = \"tmpfile2.txt\"\n",
    "tmpfile3 = \"tmpfile3.txt\"\n",
    "ids = [1, 2, 4, 21, 22, 26, 27, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crns = {\n",
    "#      1: {\"remotepattern\": \"up1_Data*.001*.txt\",\n",
    "#          \"sdpattern\": \"*.001\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"counts2\", \"nsecs2\", \"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\"]\n",
    "#         },\n",
    "\n",
    "#      2: {\"remotepattern\": \"up2_Data*.002*.txt\",\n",
    "#          \"sdpattern\": \"*.002\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "#         },\n",
    "\n",
    "#      4: {\"remotepattern\": \"up4_Data*.004*.txt\",\n",
    "#          \"sdpattern\": \"*.004\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\",\n",
    "#                       \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\", \"MetOne092_1\",\"press4\",\"temp_ext\",\n",
    "#                       \"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\",\n",
    "#                       \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\", \"MetOne092_1\",\"press4\",\"temp_ext\",\n",
    "#                       \"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\",\"sdiAdr\",\"sdi2_1\",\"sdi2_2\",\"sdi2_3\",\"sdi2_4\"]\n",
    "#         },\n",
    "\n",
    "#     21: {\"remotepattern\": \"CRSProbe_Data*.021*.txt\",\n",
    "#           \"sdpattern\": \"*.021\",\n",
    "#           \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\"],\n",
    "#           \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "#         },\n",
    "\n",
    "#     22: {\"remotepattern\": \"CRSProbe_Data*.022*.txt\",\n",
    "#          \"sdpattern\": \"*.022\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"],\n",
    "#          \"colnames3\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"countsloc1\", \"nsecsloc1\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdrloc1\",\"sdiloc1_1\",\"sdiloc1_2\",\"sdiloc1_3\",\"sdiloc1_4\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "#         },\n",
    "\n",
    "#     26: {\"remotepattern\": \"up26_Data*.026*.txt\",\n",
    "#          \"sdpattern\": \"*.026\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"]\n",
    "#         },\n",
    "\n",
    "#     27: {\"remotepattern\": \"up27_Data*.027*.txt\",\n",
    "#          \"sdpattern\": \"*.027\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\",\n",
    "#                       \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\",\n",
    "#                       \"volt\", \"counts1\", \"nsecs1\", \"counts3\", \"nsecs3\", \"counts2\", \"nsecs2\"]\n",
    "\n",
    "#         },\n",
    "\n",
    "#     28: {\"remotepattern\": \"sonde28_Data_*.028*\",\n",
    "#          \"sdpattern\": \"*.028\",\n",
    "#          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\"],\n",
    "#          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\"]\n",
    "#         }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all column descriptors FROM FILE\n",
    "# for i, id in enumerate(ids):\n",
    "#     print(\"-------------\")\n",
    "#     print(\"Processing %d\" % id)\n",
    "\n",
    "#     # REMOTE FILES\n",
    "#     print(\"   Remote: \", end=\"\")\n",
    "#     searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "#     remotefiles = glob.glob(searchdir, recursive=True)\n",
    "#     print(\"found %d files\" % len(remotefiles), end=\"\")\n",
    "    \n",
    "#     allcols = []\n",
    "#     numwithcols = 0\n",
    "\n",
    "#     for name in remotefiles:\n",
    "#         foundcol = False\n",
    "#         #print(\".\", end=\"\")\n",
    "#         with open(name, 'r') as fin:\n",
    "#             numiterations = 0\n",
    "#             while(numiterations<10000):\n",
    "#                 line = fin.readline()\n",
    "#                 # EOF\n",
    "#                 if not line:\n",
    "#                     fin.close()\n",
    "#                     break\n",
    "#                 if \"RecordNum\" in line:\n",
    "#                     # This is the colummn descriptor line\n",
    "#                     foundcol = True\n",
    "#                     line = line.strip(\",\")\n",
    "#                     line = line.replace(\",\\r\\n\", \"\")\n",
    "#                     line = line.replace(\",\\n\", \"\")\n",
    "#                     line = line.strip(\"//\")\n",
    "#                     fcols = line.split(\",\")\n",
    "#                     numcols = len(fcols)\n",
    "#                     for fcol in fcols:\n",
    "#                         fcol = fcol.strip(\"\\n\").strip(\"//\")\n",
    "#                         if not fcol==\"\":\n",
    "#                             allcols.append(fcol)\n",
    "#                     fin.close()\n",
    "#                     break\n",
    "#             numwithcols += foundcol\n",
    "                \n",
    "#     crns[id][\"allcols\"] = np.unique(allcols)\n",
    "#     crns[id][\"numfiles\"] = len(remotefiles)\n",
    "#     crns[id][\"numnocols\"] = len(remotefiles) - numwithcols\n",
    "#     print(\", no column specs for %d files\" % (len(remotefiles) - numwithcols) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Processing 1\n",
      "   Remote: found 1930 files\n",
      "-------------\n",
      "Processing 2\n",
      "   Remote: found 1484 files\n",
      "-------------\n",
      "Processing 4\n",
      "   Remote: found 2543 files\n",
      "-------------\n",
      "Processing 21\n",
      "   Remote: found 2651 files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 255: expected 20 fields, saw 30\\nSkipping line 272: expected 20 fields, saw 29\\nSkipping line 280: expected 20 fields, saw 21\\nSkipping line 315: expected 20 fields, saw 30\\nSkipping line 323: expected 20 fields, saw 29\\nSkipping line 331: expected 20 fields, saw 29\\nSkipping line 348: expected 20 fields, saw 34\\nSkipping line 356: expected 20 fields, saw 33\\nSkipping line 364: expected 20 fields, saw 31\\nSkipping line 381: expected 20 fields, saw 24\\nSkipping line 389: expected 20 fields, saw 30\\nSkipping line 406: expected 20 fields, saw 31\\nSkipping line 423: expected 20 fields, saw 26\\nSkipping line 431: expected 20 fields, saw 27\\nSkipping line 439: expected 20 fields, saw 37\\nSkipping line 447: expected 20 fields, saw 31\\nSkipping line 455: expected 20 fields, saw 27\\nSkipping line 463: expected 20 fields, saw 22\\nSkipping line 472: expected 20 fields, saw 28\\nSkipping line 480: expected 20 fields, saw 22\\nSkipping line 489: expected 20 fields, saw 22\\nSkipping line 497: expected 20 fields, saw 28\\nSkipping line 505: expected 20 fields, saw 26\\nSkipping line 514: expected 20 fields, saw 24\\nSkipping line 522: expected 20 fields, saw 31\\nSkipping line 530: expected 20 fields, saw 27\\nSkipping line 538: expected 20 fields, saw 23\\nSkipping line 556: expected 20 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Processing 22\n",
      "   Remote: found 2470 files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8: expected 28 fields, saw 29\\nSkipping line 9: expected 28 fields, saw 29\\nSkipping line 10: expected 28 fields, saw 29\\nSkipping line 11: expected 28 fields, saw 29\\nSkipping line 12: expected 28 fields, saw 29\\nSkipping line 13: expected 28 fields, saw 29\\nSkipping line 14: expected 28 fields, saw 29\\nSkipping line 15: expected 28 fields, saw 29\\nSkipping line 17: expected 28 fields, saw 29\\nSkipping line 18: expected 28 fields, saw 29\\nSkipping line 19: expected 28 fields, saw 29\\nSkipping line 21: expected 28 fields, saw 29\\nSkipping line 22: expected 28 fields, saw 29\\nSkipping line 23: expected 28 fields, saw 29\\n'\n",
      "b'Skipping line 10: expected 23 fields, saw 27\\nSkipping line 11: expected 23 fields, saw 27\\nSkipping line 12: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 7: expected 24 fields, saw 27\\nSkipping line 8: expected 24 fields, saw 27\\nSkipping line 9: expected 24 fields, saw 27\\nSkipping line 10: expected 24 fields, saw 27\\nSkipping line 11: expected 24 fields, saw 27\\nSkipping line 12: expected 24 fields, saw 27\\nSkipping line 13: expected 24 fields, saw 27\\nSkipping line 14: expected 24 fields, saw 27\\nSkipping line 15: expected 24 fields, saw 27\\nSkipping line 16: expected 24 fields, saw 27\\nSkipping line 17: expected 24 fields, saw 27\\nSkipping line 18: expected 24 fields, saw 27\\nSkipping line 19: expected 24 fields, saw 27\\nSkipping line 20: expected 24 fields, saw 27\\nSkipping line 21: expected 24 fields, saw 27\\nSkipping line 22: expected 24 fields, saw 27\\nSkipping line 23: expected 24 fields, saw 27\\n'\n",
      "b'Skipping line 11: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 7: expected 23 fields, saw 27\\nSkipping line 8: expected 23 fields, saw 27\\nSkipping line 9: expected 23 fields, saw 27\\nSkipping line 10: expected 23 fields, saw 27\\nSkipping line 11: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 9: expected 23 fields, saw 27\\nSkipping line 10: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 24\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 9: expected 23 fields, saw 26\\nSkipping line 10: expected 23 fields, saw 26\\nSkipping line 12: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 11: expected 23 fields, saw 26\\nSkipping line 22: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 10: expected 23 fields, saw 26\\nSkipping line 18: expected 23 fields, saw 26\\nSkipping line 19: expected 23 fields, saw 26\\nSkipping line 20: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 12: expected 23 fields, saw 26\\nSkipping line 13: expected 23 fields, saw 26\\nSkipping line 14: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 15: expected 23 fields, saw 26\\nSkipping line 16: expected 23 fields, saw 26\\nSkipping line 17: expected 23 fields, saw 26\\nSkipping line 18: expected 23 fields, saw 26\\nSkipping line 19: expected 23 fields, saw 26\\nSkipping line 20: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 16: expected 26 fields, saw 27\\nSkipping line 17: expected 26 fields, saw 27\\nSkipping line 18: expected 26 fields, saw 27\\nSkipping line 19: expected 26 fields, saw 27\\nSkipping line 20: expected 26 fields, saw 27\\nSkipping line 21: expected 26 fields, saw 27\\nSkipping line 22: expected 26 fields, saw 27\\nSkipping line 23: expected 26 fields, saw 27\\n'\n",
      "b'Skipping line 22: expected 26 fields, saw 27\\nSkipping line 23: expected 26 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Processing 26\n",
      "   Remote: found 2082 files\n",
      "-------------\n",
      "Processing 27\n",
      "   Remote: found 1770 files\n",
      "-------------\n",
      "Processing 28\n",
      "   Remote: found 814 files\n"
     ]
    }
   ],
   "source": [
    "crns = {\n",
    "     1: {\"remotepattern\": \"up1_Data*.001*.txt\",\n",
    "        },\n",
    "\n",
    "     2: {\"remotepattern\": \"up2_Data*.002*.txt\",\n",
    "        },\n",
    "\n",
    "     4: {\"remotepattern\": \"up4_Data*.004*.txt\",\n",
    "        },\n",
    "\n",
    "    21: {\"remotepattern\": \"CRSProbe_Data*.021*.txt\",\n",
    "        },\n",
    "\n",
    "    22: {\"remotepattern\": \"CRSProbe_Data*.022*.txt\",\n",
    "        },\n",
    "\n",
    "    26: {\"remotepattern\": \"up26_Data*.026*.txt\",\n",
    "        },\n",
    "\n",
    "    27: {\"remotepattern\": \"up27_Data*.027*.txt\",\n",
    "        },\n",
    "\n",
    "    28: {\"remotepattern\": \"sonde28_Data_*.028*\",\n",
    "        }\n",
    "}\n",
    "\n",
    "# Find all column changes FROM FILE\n",
    "for i, id in enumerate(ids):\n",
    "    print(\"-------------\")\n",
    "    print(\"Processing %d\" % id)\n",
    "\n",
    "    # REMOTE FILES\n",
    "    print(\"   Remote: \", end=\"\")\n",
    "    searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "    remotefiles = sorted(glob.glob(searchdir, recursive=True))\n",
    "    print(\"found %d files\" % len(remotefiles), end=\"\")\n",
    "    \n",
    "    numcols = 0\n",
    "    newnumcols = []\n",
    "    newcolsdate = []\n",
    "    newcolsfile = []\n",
    "\n",
    "    for name in remotefiles:\n",
    "        foundcol = False\n",
    "        # Clean file content\n",
    "        with open(name, 'r') as fin:\n",
    "            body = fin.read()\n",
    "            # replace comment character\n",
    "            body = body.replace(\"//\", \"#\")\n",
    "            # replace zombie line endings\n",
    "            body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "            # comment out these lines\n",
    "            body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "            body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "            fin.close()\n",
    "        # and write it to tmpfile\n",
    "        with open(tmpfile, 'w') as fout:\n",
    "            fout.write(body)\n",
    "        # Read again as Pandas dataframe\n",
    "        try:\n",
    "            df = pd.read_csv(tmpfile, sep=\",\", comment=\"#\", header=None, error_bad_lines=False, \n",
    "                             warn_bad_lines=True)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "        if len(df.columns) < 7:\n",
    "            continue            \n",
    "        try:\n",
    "            df[1] = pd.to_datetime(df[1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "        except:\n",
    "            dtmask = np.repeat(True,len(df))\n",
    "            for k in range(len(df)):\n",
    "                try:\n",
    "                    _ = pd.to_datetime(df.loc[k,1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "                except ValueError: \n",
    "                    dtmask[k] = False\n",
    "            df2 = df.loc[dtmask]\n",
    "            if len(df2)==0:\n",
    "                sys.exit()\n",
    "            df = df2\n",
    "            df[1] = pd.to_datetime(df[1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "        if len(df.columns) != numcols:\n",
    "                newnumcols.append(len(df.columns))\n",
    "                newcolsdate.append(df[1][0])\n",
    "                newcolsfile.append(os.path.basename(name))\n",
    "                numcols=len(df.columns)\n",
    "    crns[id][\"setup\"] = pd.DataFrame({\"datetime\":newcolsdate, \"file\": newcolsfile, \"numcols\": newnumcols})\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "1\n",
      "             datetime                               file  numcols\n",
      "0 2019-07-25 10:39:58  up1_Data1907251039.001_000001.txt       10\n",
      "1 2020-04-15 10:23:28  up1_Data2004151023.001_000001.txt       15\n",
      "2 2021-01-19 09:28:36  up1_Data2101190928.001_000002.txt       14\n",
      "3 2021-01-25 12:02:02  up1_Data2101251202.001_000001.txt       15\n",
      "\n",
      "======================================================\n",
      "2\n",
      "             datetime                                file  numcols\n",
      "0 2019-07-31 12:37:38  up2_Data_1907311237.002_000001.txt        8\n",
      "1 2020-04-07 11:01:39  up2_Data_2004071101.002_000002.txt       15\n",
      "2 2021-04-19 10:39:18  up2_Data_2104191038.002_000002.txt       10\n",
      "3 2021-06-04 11:11:34  up2_Data_2106041111.002_000002.txt       15\n",
      "\n",
      "======================================================\n",
      "4\n",
      "             datetime                                file  numcols\n",
      "0 2019-08-06 13:25:33  up4_Data_1908061325.004_000001.txt       18\n",
      "1 2020-03-23 13:03:24  up4_Data_2003231303.004_000001.txt       23\n",
      "\n",
      "======================================================\n",
      "21\n",
      "             datetime                                    file  numcols\n",
      "0 2019-07-18 09:58:42  CRSProbe_Data1907180958.021_000001.txt       13\n",
      "1 2020-04-07 10:05:40  CRSProbe_Data2004071005.021_000001.txt       20\n",
      "\n",
      "======================================================\n",
      "22\n",
      "              datetime                                    file  numcols\n",
      "0  2019-07-25 09:05:48  CRSProbe_Data1907250905.022_000001.txt       13\n",
      "1  2020-04-07 12:46:48  CRSProbe_Data2004071246.022_000002.txt       20\n",
      "2  2020-07-08 10:53:00  CRSProbe_Data2007081052.022_000002.txt       17\n",
      "3  2020-07-08 11:11:44  CRSProbe_Data2007081111.022_000001.txt       28\n",
      "4  2020-07-08 13:55:22  CRSProbe_Data2007081355.022_000022.txt       29\n",
      "5  2020-07-09 14:14:00  CRSProbe_Data2007081355.022_000094.txt       28\n",
      "6  2020-07-09 20:14:00  CRSProbe_Data2007081355.022_000112.txt       29\n",
      "7  2020-07-10 10:32:00  CRSProbe_Data2007101031.022_000023.txt       27\n",
      "8  2020-09-25 09:34:00  CRSProbe_Data2009140904.022_000813.txt       23\n",
      "9  2020-09-25 15:34:00  CRSProbe_Data2009140904.022_000831.txt       27\n",
      "10 2020-10-21 15:34:00  CRSProbe_Data2010010014.022_002703.txt       24\n",
      "11 2020-10-21 21:34:00  CRSProbe_Data2010010014.022_002721.txt       27\n",
      "12 2020-10-24 21:34:00  CRSProbe_Data2010010014.022_002937.txt       23\n",
      "13 2020-10-30 15:34:00  CRSProbe_Data2010010014.022_003351.txt       27\n",
      "14 2020-10-30 21:34:00  CRSProbe_Data2010010014.022_003369.txt       23\n",
      "15 2020-10-31 03:34:00  CRSProbe_Data2010010014.022_003387.txt       27\n",
      "16 2020-10-31 09:34:00  CRSProbe_Data2010010014.022_003405.txt       23\n",
      "17 2020-10-31 15:34:00  CRSProbe_Data2010010014.022_003423.txt       27\n",
      "18 2020-11-01 15:34:00  CRSProbe_Data2011010014.022_003495.txt       23\n",
      "19 2020-11-04 09:17:54  CRSProbe_Data2011040917.022_000023.txt       26\n",
      "20 2020-11-10 15:37:00  CRSProbe_Data2011040917.022_000477.txt       27\n",
      "21 2020-11-15 09:37:00  CRSProbe_Data2011040917.022_000819.txt       26\n",
      "22 2020-11-20 15:37:00  CRSProbe_Data2011040917.022_001202.txt       27\n",
      "23 2020-12-12 22:37:00  CRSProbe_Data2012010017.022_002807.txt       26\n",
      "24 2021-01-25 12:40:40  CRSProbe_Data2101251240.022_000001.txt       17\n",
      "25 2021-01-25 12:52:46  CRSProbe_Data2101251252.022_000002.txt       20\n",
      "\n",
      "======================================================\n",
      "26\n",
      "             datetime                                 file  numcols\n",
      "0 2019-08-23 10:20:31  up26_Data_1908231020.026_000003.txt       11\n",
      "1 2020-07-08 08:07:42  up26_Data_2007080807.026_000001.txt        7\n",
      "2 2020-07-08 08:23:28  up26_Data_2007080823.026_000003.txt        9\n",
      "\n",
      "======================================================\n",
      "27\n",
      "             datetime                                 file  numcols\n",
      "0 2019-10-22 12:09:05  up27_Data_1910221208.027_000003.txt       10\n",
      "1 2019-12-12 09:59:42  up27_Data_1912120959.027_000001.txt       12\n",
      "\n",
      "======================================================\n",
      "28\n",
      "             datetime                                    file  numcols\n",
      "0 2019-11-21 11:09:47  sonde28_Data_1911211109.028_000002.txt       11\n",
      "1 2020-04-07 13:26:45  sonde28_Data_2004071326.028_000002.txt       16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in crns.keys():\n",
    "    print(\"======================================================\")\n",
    "    print(key)\n",
    "    print(crns[key][\"setup\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/remote/21/CRSProbe_Data2104010004.021_027577.txt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to maintenance events    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "# 1\n",
    "\n",
    "# Start\n",
    "0 \"2019-07-25 10:39:58\"  up1_Data1907251039.001_000001.txt       10\n",
    "//RecordNum,Date Time(UTC),P1_mb,T1_C,RH1,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,\n",
    "# Add PR-2\n",
    "1 \"2020-04-15 10:23:28\"  up1_Data2004151023.001_000001.txt       15\n",
    "//RecordNum,Date Time(UTC),P1_mb,T1_C,RH1,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdi1_4,\n",
    "\n",
    "# 2020-07-08 08:55:00 --> CRNS 1 is logged via CRNS 22\n",
    "\n",
    "# CRNS 1 is logged at CRNS 1 again, but one PR-2 depth missing\n",
    "2 \"2021-01-19 09:28:36\"  up1_Data2101190928.001_000002.txt       14\n",
    "//RecordNum,Date Time(UTC),P1_mb,T1_C,RH1,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,sdiAdr,sdi1_1,sdi1_2,sdi1_3,\n",
    "# PR-2 complete again\n",
    "3 \"2021-01-25 12:02:02\"  up1_Data2101251202.001_000001.txt       15  \n",
    "//RecordNum,Date Time(UTC),P1_mb,T1_C,RH1,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdi1_4,\n",
    "\n",
    "\n",
    "# ------\n",
    "# 2\n",
    "\n",
    "# Start\n",
    "0 \"2019-07-31 12:37:38\"  up2_Data_1907311237.002_000001.txt        8\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\"\n",
    "# Add PR-2\n",
    "1 \"2020-04-07 11:01:39\"  up2_Data_2004071101.002_000002.txt       15\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"\n",
    "# New Modem, but output corrupt\n",
    "2 \"2021-04-19 10:39:18\"  up2_Data_2104191038.002_000002.txt       10\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"sdiAdr\",\"trash\"\n",
    "# Modem adjusted\n",
    "3 \"2021-06-04 11:11:34\"  up2_Data_2106041111.002_000002.txt       15\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"\n",
    "# 14/06/2021 08:13:00 --> removed probe...WHERE?\n",
    "\n",
    "# ------\n",
    "# 4\n",
    "\n",
    "# Start\n",
    "0 \"2019-08-06 13:25:33\"  up4_Data_1908061325.004_000001.txt       18\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\",\n",
    "\"MetOne092_1\",\"press4\",\"temp_ext\",\"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\"\n",
    "\n",
    "# Add PR2/4 on 2020-03-16, but logging only works after 2020-03-23\n",
    "1 \"2020-03-23 13:03:24\"  up4_Data_2003231303.004_000001.txt       23\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\",\n",
    "\"MetOne092_1\",\"press4\",\"temp_ext\", \"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\",\n",
    "\"sdiAdr\",\"sdi2_1\",\"sdi2_2\",\"sdi2_3\",\"sdi2_4\"\n",
    "\n",
    "        \n",
    "# ------\n",
    "# 21\n",
    "\n",
    "# Start\n",
    "0 \"2019-07-18 09:58:42\"  CRSProbe_Data1907180958.021_000001.txt       13\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\",\n",
    "\"N1T_C\", \"N1RH\"\n",
    "\n",
    "# Add PR-2\n",
    "1 \"2020-04-07 10:05:40\"  CRSProbe_Data2004071005.021_000001.txt       20\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\",\n",
    "\"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"\n",
    "\n",
    "# ------\n",
    "# 22\n",
    "\n",
    "22\n",
    "\n",
    "# Start\n",
    "0  \"2019-07-25 09:05:48\"  CRSProbe_Data1907250905.022_000001.txt       13\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\"relhum_ext\", \"volt\",\n",
    "\"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\"\n",
    "#          \"colnames3\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "#                        \"relhum_ext\", \"volt\", \"countsloc1\", \"nsecsloc1\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdrloc1\",\"sdiloc1_1\",\"sdiloc1_2\",\"sdiloc1_3\",\"sdiloc1_4\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "#         },\n",
    "# Add PR2/6\n",
    "1  \"2020-04-07 12:46:48\"  CRSProbe_Data2004071246.022_000002.txt       20\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\"relhum_ext\", \"volt\", \n",
    "\"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"\n",
    "# Preparing link to sensor 1?\n",
    "2  \"2020-07-08 10:53:00\"  CRSProbe_Data2007081052.022_000002.txt       17\n",
    "//RecordNum,Date Time(UTC),P1_mb,P4_mb,T1_C,RH1,T_CS215,RH_CS215,Vbat,N1Cts,N1ET_sec,N1T_C,N1RH,sdiAdr,sdi1_1,sdi1_2,sdi1_3,\n",
    "# Link to sensor 1: N1e1 Röhre 1 von #1; n2e2 Röhre 2 von #1; n3e3 Röhre von #22; s3 interner Sensor Röhre #22. z1m! PR2 von #1; z2m! PR2 von #22.\n",
    "3  \"2020-07-08 11:11:44\"  CRSProbe_Data2007081111.022_000001.txt       28\n",
    "//RecordNum,Date Time(UTC),P1_mb,P4_mb,T1_C,RH1,T_CS215,RH_CS215,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,N3Cts,N3ET_sec,N3T_C,N3RH,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdiAdr,sdi2_1,sdi2_2,sdi2_3,sdi2_4,sdi2_5,sdi2_6,\n",
    "# Try to fix link to sensor 1: sdi1_4 was still missing...?\n",
    "4  \"2020-07-08 13:55:22\"  CRSProbe_Data2007081355.022_000022.txt       29\n",
    "//RecordNum,Date Time(UTC),P1_mb,P4_mb,T1_C,RH1,T_CS215,RH_CS215,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,N3Cts,N3ET_sec,N3T_C,N3RH,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdi1_4,sdiAdr,sdi2_1,sdi2_2,sdi2_3,sdi2_4,sdi2_5,sdi2_6,\n",
    "# No maintenence event in logbook, header has 29 cols, first two rows only 28, then to 29 again...\n",
    "5  \"2020-07-09 14:14:00\"  CRSProbe_Data2007081355.022_000094.txt       28\n",
    "//RecordNum,Date Time(UTC),P1_mb,P4_mb,T1_C,RH1,T_CS215,RH_CS215,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,N3Cts,N3ET_sec,N3T_C,N3RH,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdi1_4,sdiAdr,sdi2_1,sdi2_2,sdi2_3,sdi2_4,sdi2_5,sdi2_6,\n",
    "# No maintenance event in logbook, back to 29 columns as should be?\n",
    "6  \"2020-07-09 20:14:00\"  CRSProbe_Data2007081355.022_000112.txt       29\n",
    "//RecordNum,Date Time(UTC),P1_mb,P4_mb,T1_C,RH1,T_CS215,RH_CS215,Vbat,N1Cts,N1ET_sec,N2Cts,N2ET_sec,N3Cts,N3ET_sec,N3T_C,N3RH,sdiAdr,sdi1_1,sdi1_2,sdi1_3,sdi1_4,sdiAdr,sdi2_1,sdi2_2,sdi2_3,sdi2_4,sdi2_5,sdi2_6,\n",
    "7  \"2020-07-10 10:32:00\"  CRSProbe_Data2007101031.022_000023.txt       27\n",
    "8  \"2020-09-25 09:34:00\"  CRSProbe_Data2009140904.022_000813.txt       23\n",
    "9  \"2020-09-25 15:34:00\"  CRSProbe_Data2009140904.022_000831.txt       27\n",
    "10 \"2020-10-21 15:34:00\"  CRSProbe_Data2010010014.022_002703.txt       24\n",
    "11 \"2020-10-21 21:34:00\"  CRSProbe_Data2010010014.022_002721.txt       27\n",
    "12 \"2020-10-24 21:34:00\"  CRSProbe_Data2010010014.022_002937.txt       23\n",
    "13 \"2020-10-30 15:34:00\"  CRSProbe_Data2010010014.022_003351.txt       27\n",
    "14 \"2020-10-30 21:34:00\"  CRSProbe_Data2010010014.022_003369.txt       23\n",
    "15 \"2020-10-31 03:34:00\"  CRSProbe_Data2010010014.022_003387.txt       27\n",
    "16 \"2020-10-31 09:34:00\"  CRSProbe_Data2010010014.022_003405.txt       23\n",
    "17 \"2020-10-31 15:34:00\"  CRSProbe_Data2010010014.022_003423.txt       27\n",
    "18 \"2020-11-01 15:34:00\"  CRSProbe_Data2011010014.022_003495.txt       23\n",
    "19 \"2020-11-04 09:17:54\"  CRSProbe_Data2011040917.022_000023.txt       26\n",
    "20 \"2020-11-10 15:37:00\"  CRSProbe_Data2011040917.022_000477.txt       27\n",
    "21 \"2020-11-15 09:37:00\"  CRSProbe_Data2011040917.022_000819.txt       26\n",
    "22 \"2020-11-20 15:37:00\"  CRSProbe_Data2011040917.022_001202.txt       27\n",
    "23 \"2020-12-12 22:37:00\"  CRSProbe_Data2012010017.022_002807.txt       26\n",
    "24 \"2021-01-25 12:40:40\"  CRSProbe_Data2101251240.022_000001.txt       17\n",
    "25 \"2021-01-25 12:52:46\"  CRSProbe_Data2101251252.022_000002.txt       20\n",
    "\n",
    "\n",
    "# ------\n",
    "#26\n",
    "\n",
    "# Start\n",
    "0 \"2019-08-23 10:20:31\"  up26_Data_1908231020.026_000003.txt       11\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"\n",
    "# Removed internal tube (CRS 1000, former Brazil?), after first attempt to restart\n",
    "1 \"2020-07-08 08:07:42\"  up26_Data_2007080807.026_000001.txt        7\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\"\n",
    "# ... after second attempt to restart\n",
    "2 \"2020-07-08 08:23:28\"  up26_Data_2007080823.026_000003.txt        9\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts2\", \"nsecs2\"\n",
    "# ATTENTION: uninstalled on 2020-08-19 for moving to Wüstebach, back from Wüstebach on 2020-11-18\n",
    "\n",
    "\n",
    "# ------\n",
    "# 27\n",
    "\n",
    "# Start with moderated CRS 1000 (counts1) and Lab-C (counts2)\n",
    "0 \"2019-10-22 12:09:05\"  up27_Data_1910221208.027_000003.txt       10\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"\n",
    "# Add bare conter as counts3\n",
    "1 \"2019-12-12 09:59:42\"  up27_Data_1912120959.027_000001.txt       12\n",
    "\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts3\", \"nsecs3\", \"counts2\", \"nsecs2\"\n",
    "\n",
    "# ------\n",
    "# 28\n",
    "\n",
    "# Start\n",
    "0 \"2019-11-21 11:09:47\"  sonde28_Data_1911211109.028_000002.txt       11\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\"\n",
    "# Add PR2/4\n",
    "1 \"2020-04-07 13:26:45\"  sonde28_Data_2004071326.028_000002.txt       16\n",
    "\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\"\n",
    "# ATTENTION: Uninstalled on 2020-06-11 (for moving to Wüstebach??), back from Wüstebach on 2020-11-18\n",
    "#            uninstalled on 2021-06-14 for moving to...Rhinluch? Sponheim? Other location?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(name, 'r') as file:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Processing 27\n",
      "Remote: found 329 files\n",
      ".........................................................................................................................................................................................................................................................................................................................................\n",
      "SD: found 768 files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for i, id in enumerate(ids):\n",
    "    print(\"-------------\")\n",
    "    print(\"Processing %d\" % id)\n",
    "\n",
    "    try:\n",
    "        os.remove(tmpfile)\n",
    "        os.remove(tmpfile2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # REMOTE FILES\n",
    "    print(\"Remote: \", end=\"\")\n",
    "    searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "    remotefiles = glob.glob(searchdir, recursive=True)\n",
    "    print(\"found %d files\" % len(remotefiles))\n",
    "\n",
    "    for name in remotefiles:\n",
    "        print(\".\", end=\"\")\n",
    "        fin = open(name, \"r\")\n",
    "        body = fin.read()\n",
    "        # replace comment character\n",
    "        body = body.replace(\"//\", \"#\")\n",
    "        # replace zombie line endings\n",
    "        body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "        # comment out these lines\n",
    "        body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "        body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "        myfile = open(tmpfile, 'a')\n",
    "        myfile.write(body)\n",
    "        myfile.close()\n",
    "    print(\"\")\n",
    "\n",
    "    # SD\n",
    "    print(\"SD: \", end=\"\")\n",
    "    searchdir = os.path.join(sddir, \"%d\" % id)\n",
    "    sdfiles = [filename for filename in Path(searchdir).glob(\"**/\"+crns[id][\"sdpattern\"])]\n",
    "    print(\"found %d files\" % len(sdfiles))\n",
    "\n",
    "    for name in sdfiles:\n",
    "        print(\".\", end=\"\")\n",
    "        fin = open(name, \"r\")\n",
    "        body = fin.read()\n",
    "        # replace comment character\n",
    "        body = body.replace(\"//\", \"#\")\n",
    "        # replace zombie line endings\n",
    "        body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "        body = body.replace(\",\\n\", \"\\n\")\n",
    "        # comment out these lines\n",
    "        body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "        body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "        myfile = open(tmpfile, 'a')\n",
    "        myfile.write(body)\n",
    "        myfile.close()\n",
    "    print(\"\")\n",
    "\n",
    "    if (\"colnames2\" in crns[id].keys()) or (\"colnames3\" in crns[id].keys()):\n",
    "        # Read all lines. potentially varying no of columns\n",
    "        myfile = open(tmpfile, 'r')\n",
    "        lines = myfile.readlines()\n",
    "        myfile.close()\n",
    "        # Write in seperate files\n",
    "        myfile = open(tmpfile, 'w')\n",
    "        myfile2 = open(tmpfile2, 'w')\n",
    "        myfile3 = open(tmpfile3, 'w')\n",
    "        for line in lines:\n",
    "            split = line.split(\",\")\n",
    "            if len(split)==len(crns[id][\"colnames\"]):\n",
    "                myfile.write(line+\"\\n\")\n",
    "            if len(split)==len(crns[id][\"colnames2\"]):\n",
    "                myfile2.write(line+\"\\n\")\n",
    "            try:\n",
    "                if len(split)==len(crns[id][\"colnames3\"]):\n",
    "                    myfile3.write(line+\"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "        myfile.close()\n",
    "        myfile2.close()\n",
    "        try:\n",
    "            myfile3.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # MERGE\n",
    "    df = pd.read_csv(tmpfile, sep=\",\", comment=\"#\", header=None, error_bad_lines=False, warn_bad_lines=True)\n",
    "    df.columns = crns[id][\"colnames\"]\n",
    "    if \"colnames2\" in crns[id].keys():\n",
    "        try:\n",
    "            df2 = pd.read_csv(tmpfile2, sep=\",\", comment=\"#\", header=None,\n",
    "                             error_bad_lines=False, warn_bad_lines=True)\n",
    "            df2.columns = crns[id][\"colnames2\"]\n",
    "            df = df2.append(df, sort=False)\n",
    "        except:\n",
    "            print(\"Problem in reading or appending data with diffferent column scenario\")\n",
    "            raise\n",
    "    if \"colnames3\" in crns[id].keys():\n",
    "        try:\n",
    "            df3 = pd.read_csv(tmpfile3, sep=\",\", comment=\"#\", header=None,\n",
    "                             error_bad_lines=False, warn_bad_lines=True)\n",
    "            df3.columns = crns[id][\"colnames3\"]\n",
    "            df = df3.append(df, sort=False)\n",
    "        except:\n",
    "            print(\"Problem in reading or appending data with diffferent column scenario\")\n",
    "            raise\n",
    "    df.datetime = pd.to_datetime(df.datetime, format=\"%Y/%m/%d %H:%M:%S\")\n",
    "    df = df.set_index(\"datetime\")\n",
    "    df.insert(loc=1, column=\"datetime\", value=df.index)\n",
    "    df = df.sort_index()\n",
    "    df = df[df.index >= \"2019-07-25\"]\n",
    "    dupl = df.index.duplicated(keep='first')\n",
    "    if np.any(dupl):\n",
    "        print(\"Contains %d duplicates\" % len(np.where(dupl)[0]))\n",
    "        df = df[~dupl]\n",
    "    fpath = os.path.join(trgdir, \"%d/%d_CRNS.txt\" % (id, id) )\n",
    "    df.to_csv(fpath, sep=\"\\t\", index=False, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
