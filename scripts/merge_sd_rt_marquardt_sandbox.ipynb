{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "sddir = \"/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/sd\"\n",
    "remotedir = \"/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/remote\"\n",
    "trgdir = \"/media/x/cosmicsense/data/marquardt/crns\"\n",
    "tmpfile = \"tmpfile.txt\"\n",
    "tmpfile2 = \"tmpfile2.txt\"\n",
    "tmpfile3 = \"tmpfile3.txt\"\n",
    "ids = [1, 2, 4, 21, 22, 26, 27, 28]\n",
    "\n",
    "crns = {\n",
    "     1: {\"remotepattern\": \"up1_Data*.001*.txt\",\n",
    "         \"sdpattern\": \"*.001\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"counts2\", \"nsecs2\", \"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\"]\n",
    "        },\n",
    "\n",
    "     2: {\"remotepattern\": \"up2_Data*.002*.txt\",\n",
    "         \"sdpattern\": \"*.002\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "        },\n",
    "\n",
    "     4: {\"remotepattern\": \"up4_Data*.004*.txt\",\n",
    "         \"sdpattern\": \"*.004\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\",\n",
    "                      \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\", \"MetOne092_1\",\"press4\",\"temp_ext\",\n",
    "                      \"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\", \"volt\",\n",
    "                      \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\", \"MetOne092_1\",\"press4\",\"temp_ext\",\n",
    "                      \"relhum_ext\",\"N1T_C\",\"N1RH\",\"N2T_C\",\"N2RH\",\"sdiAdr\",\"sdi2_1\",\"sdi2_2\",\"sdi2_3\",\"sdi2_4\"]\n",
    "        },\n",
    "\n",
    "    21: {\"remotepattern\": \"CRSProbe_Data*.021*.txt\",\n",
    "          \"sdpattern\": \"*.021\",\n",
    "          \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "                       \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\"],\n",
    "          \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "                       \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "        },\n",
    "\n",
    "    22: {\"remotepattern\": \"CRSProbe_Data*.022*.txt\",\n",
    "         \"sdpattern\": \"*.022\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "                       \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "                       \"relhum_ext\", \"volt\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"],\n",
    "         \"colnames3\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\",\"relhum1\", \"temp_ext\",\n",
    "                       \"relhum_ext\", \"volt\", \"countsloc1\", \"nsecsloc1\", \"counts1\", \"nsecs1\", \"N1T_C\", \"N1RH\",\"sdiAdrloc1\",\"sdiloc1_1\",\"sdiloc1_2\",\"sdiloc1_3\",\"sdiloc1_4\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\",\"sdi1_5\",\"sdi1_6\"]\n",
    "        },\n",
    "\n",
    "    26: {\"remotepattern\": \"up26_Data*.026*.txt\",\n",
    "         \"sdpattern\": \"*.026\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"]\n",
    "        },\n",
    "\n",
    "    27: {\"remotepattern\": \"up27_Data*.027*.txt\",\n",
    "         \"sdpattern\": \"*.027\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\",\n",
    "                      \"volt\", \"counts1\", \"nsecs1\", \"counts2\", \"nsecs2\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"temp1\", \"relhum1\",\n",
    "                      \"volt\", \"counts1\", \"nsecs1\", \"counts3\", \"nsecs3\", \"counts2\", \"nsecs2\"]\n",
    "\n",
    "        },\n",
    "\n",
    "    28: {\"remotepattern\": \"sonde28_Data_*.028*\",\n",
    "         \"sdpattern\": \"*.028\",\n",
    "         \"colnames\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\"],\n",
    "         \"colnames2\": [\"rec_id\", \"datetime\", \"press1\", \"press4\", \"temp1\", \"relhum1\", \"volt\", \"counts1\", \"nsecs1\", \"temp2\", \"relhum2\",\"sdiAdr\",\"sdi1_1\",\"sdi1_2\",\"sdi1_3\",\"sdi1_4\"]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all column descriptors FROM FILE\n",
    "for i, id in enumerate(ids):\n",
    "    print(\"-------------\")\n",
    "    print(\"Processing %d\" % id)\n",
    "\n",
    "    # REMOTE FILES\n",
    "    print(\"   Remote: \", end=\"\")\n",
    "    searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "    remotefiles = glob.glob(searchdir, recursive=True)\n",
    "    print(\"found %d files\" % len(remotefiles), end=\"\")\n",
    "    \n",
    "    allcols = []\n",
    "    numwithcols = 0\n",
    "\n",
    "    for name in remotefiles:\n",
    "        foundcol = False\n",
    "        #print(\".\", end=\"\")\n",
    "        with open(name, 'r') as fin:\n",
    "            numiterations = 0\n",
    "            while(numiterations<10000):\n",
    "                line = fin.readline()\n",
    "                # EOF\n",
    "                if not line:\n",
    "                    fin.close()\n",
    "                    break\n",
    "                if \"RecordNum\" in line:\n",
    "                    # This is the colummn descriptor line\n",
    "                    foundcol = True\n",
    "                    line = line.strip(\",\")\n",
    "                    line = line.replace(\",\\r\\n\", \"\")\n",
    "                    line = line.replace(\",\\n\", \"\")\n",
    "                    line = line.strip(\"//\")\n",
    "                    fcols = line.split(\",\")\n",
    "                    numcols = len(fcols)\n",
    "                    for fcol in fcols:\n",
    "                        fcol = fcol.strip(\"\\n\").strip(\"//\")\n",
    "                        if not fcol==\"\":\n",
    "                            allcols.append(fcol)\n",
    "                    fin.close()\n",
    "                    break\n",
    "            numwithcols += foundcol\n",
    "                \n",
    "    crns[id][\"allcols\"] = np.unique(allcols)\n",
    "    crns[id][\"numfiles\"] = len(remotefiles)\n",
    "    crns[id][\"numnocols\"] = len(remotefiles) - numwithcols\n",
    "    print(\", no column specs for %d files\" % (len(remotefiles) - numwithcols) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Processing 1\n",
      "   Remote: found 1930 files\n",
      "-------------\n",
      "Processing 2\n",
      "   Remote: found 1484 files\n",
      "-------------\n",
      "Processing 4\n",
      "   Remote: found 2543 files\n",
      "-------------\n",
      "Processing 21\n",
      "   Remote: found 2651 files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 255: expected 20 fields, saw 30\\nSkipping line 272: expected 20 fields, saw 29\\nSkipping line 280: expected 20 fields, saw 21\\nSkipping line 315: expected 20 fields, saw 30\\nSkipping line 323: expected 20 fields, saw 29\\nSkipping line 331: expected 20 fields, saw 29\\nSkipping line 348: expected 20 fields, saw 34\\nSkipping line 356: expected 20 fields, saw 33\\nSkipping line 364: expected 20 fields, saw 31\\nSkipping line 381: expected 20 fields, saw 24\\nSkipping line 389: expected 20 fields, saw 30\\nSkipping line 406: expected 20 fields, saw 31\\nSkipping line 423: expected 20 fields, saw 26\\nSkipping line 431: expected 20 fields, saw 27\\nSkipping line 439: expected 20 fields, saw 37\\nSkipping line 447: expected 20 fields, saw 31\\nSkipping line 455: expected 20 fields, saw 27\\nSkipping line 463: expected 20 fields, saw 22\\nSkipping line 472: expected 20 fields, saw 28\\nSkipping line 480: expected 20 fields, saw 22\\nSkipping line 489: expected 20 fields, saw 22\\nSkipping line 497: expected 20 fields, saw 28\\nSkipping line 505: expected 20 fields, saw 26\\nSkipping line 514: expected 20 fields, saw 24\\nSkipping line 522: expected 20 fields, saw 31\\nSkipping line 530: expected 20 fields, saw 27\\nSkipping line 538: expected 20 fields, saw 23\\nSkipping line 556: expected 20 fields, saw 23\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Processing 22\n",
      "   Remote: found 2470 files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8: expected 28 fields, saw 29\\nSkipping line 9: expected 28 fields, saw 29\\nSkipping line 10: expected 28 fields, saw 29\\nSkipping line 11: expected 28 fields, saw 29\\nSkipping line 12: expected 28 fields, saw 29\\nSkipping line 13: expected 28 fields, saw 29\\nSkipping line 14: expected 28 fields, saw 29\\nSkipping line 15: expected 28 fields, saw 29\\nSkipping line 17: expected 28 fields, saw 29\\nSkipping line 18: expected 28 fields, saw 29\\nSkipping line 19: expected 28 fields, saw 29\\nSkipping line 21: expected 28 fields, saw 29\\nSkipping line 22: expected 28 fields, saw 29\\nSkipping line 23: expected 28 fields, saw 29\\n'\n",
      "b'Skipping line 10: expected 23 fields, saw 27\\nSkipping line 11: expected 23 fields, saw 27\\nSkipping line 12: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 7: expected 24 fields, saw 27\\nSkipping line 8: expected 24 fields, saw 27\\nSkipping line 9: expected 24 fields, saw 27\\nSkipping line 10: expected 24 fields, saw 27\\nSkipping line 11: expected 24 fields, saw 27\\nSkipping line 12: expected 24 fields, saw 27\\nSkipping line 13: expected 24 fields, saw 27\\nSkipping line 14: expected 24 fields, saw 27\\nSkipping line 15: expected 24 fields, saw 27\\nSkipping line 16: expected 24 fields, saw 27\\nSkipping line 17: expected 24 fields, saw 27\\nSkipping line 18: expected 24 fields, saw 27\\nSkipping line 19: expected 24 fields, saw 27\\nSkipping line 20: expected 24 fields, saw 27\\nSkipping line 21: expected 24 fields, saw 27\\nSkipping line 22: expected 24 fields, saw 27\\nSkipping line 23: expected 24 fields, saw 27\\n'\n",
      "b'Skipping line 11: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 7: expected 23 fields, saw 27\\nSkipping line 8: expected 23 fields, saw 27\\nSkipping line 9: expected 23 fields, saw 27\\nSkipping line 10: expected 23 fields, saw 27\\nSkipping line 11: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 9: expected 23 fields, saw 27\\nSkipping line 10: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 24\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 9: expected 23 fields, saw 26\\nSkipping line 10: expected 23 fields, saw 26\\nSkipping line 12: expected 23 fields, saw 27\\nSkipping line 13: expected 23 fields, saw 27\\nSkipping line 14: expected 23 fields, saw 27\\nSkipping line 15: expected 23 fields, saw 27\\nSkipping line 16: expected 23 fields, saw 27\\nSkipping line 17: expected 23 fields, saw 27\\nSkipping line 18: expected 23 fields, saw 27\\nSkipping line 19: expected 23 fields, saw 27\\nSkipping line 20: expected 23 fields, saw 27\\nSkipping line 21: expected 23 fields, saw 27\\nSkipping line 22: expected 23 fields, saw 27\\nSkipping line 23: expected 23 fields, saw 27\\n'\n",
      "b'Skipping line 11: expected 23 fields, saw 26\\nSkipping line 22: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 10: expected 23 fields, saw 26\\nSkipping line 18: expected 23 fields, saw 26\\nSkipping line 19: expected 23 fields, saw 26\\nSkipping line 20: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 12: expected 23 fields, saw 26\\nSkipping line 13: expected 23 fields, saw 26\\nSkipping line 14: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 15: expected 23 fields, saw 26\\nSkipping line 16: expected 23 fields, saw 26\\nSkipping line 17: expected 23 fields, saw 26\\nSkipping line 18: expected 23 fields, saw 26\\nSkipping line 19: expected 23 fields, saw 26\\nSkipping line 20: expected 23 fields, saw 26\\n'\n",
      "b'Skipping line 16: expected 26 fields, saw 27\\nSkipping line 17: expected 26 fields, saw 27\\nSkipping line 18: expected 26 fields, saw 27\\nSkipping line 19: expected 26 fields, saw 27\\nSkipping line 20: expected 26 fields, saw 27\\nSkipping line 21: expected 26 fields, saw 27\\nSkipping line 22: expected 26 fields, saw 27\\nSkipping line 23: expected 26 fields, saw 27\\n'\n",
      "b'Skipping line 22: expected 26 fields, saw 27\\nSkipping line 23: expected 26 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Processing 26\n",
      "   Remote: found 2082 files\n",
      "-------------\n",
      "Processing 27\n",
      "   Remote: found 1770 files\n",
      "-------------\n",
      "Processing 28\n",
      "   Remote: found 814 files\n"
     ]
    }
   ],
   "source": [
    "crns = {\n",
    "     1: {\"remotepattern\": \"up1_Data*.001*.txt\",\n",
    "        },\n",
    "\n",
    "     2: {\"remotepattern\": \"up2_Data*.002*.txt\",\n",
    "        },\n",
    "\n",
    "     4: {\"remotepattern\": \"up4_Data*.004*.txt\",\n",
    "        },\n",
    "\n",
    "    21: {\"remotepattern\": \"CRSProbe_Data*.021*.txt\",\n",
    "        },\n",
    "\n",
    "    22: {\"remotepattern\": \"CRSProbe_Data*.022*.txt\",\n",
    "        },\n",
    "\n",
    "    26: {\"remotepattern\": \"up26_Data*.026*.txt\",\n",
    "        },\n",
    "\n",
    "    27: {\"remotepattern\": \"up27_Data*.027*.txt\",\n",
    "        },\n",
    "\n",
    "    28: {\"remotepattern\": \"sonde28_Data_*.028*\",\n",
    "        }\n",
    "}\n",
    "\n",
    "# Find all column changes FROM FILE\n",
    "for i, id in enumerate(ids):\n",
    "    print(\"-------------\")\n",
    "    print(\"Processing %d\" % id)\n",
    "\n",
    "    # REMOTE FILES\n",
    "    print(\"   Remote: \", end=\"\")\n",
    "    searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "    remotefiles = sorted(glob.glob(searchdir, recursive=True))\n",
    "    print(\"found %d files\" % len(remotefiles), end=\"\")\n",
    "    \n",
    "    numcols = 0\n",
    "    newnumcols = []\n",
    "    newcolsdate = []\n",
    "    newcolsfile = []\n",
    "\n",
    "    for name in remotefiles:\n",
    "        foundcol = False\n",
    "        # Clean file content\n",
    "        with open(name, 'r') as fin:\n",
    "            body = fin.read()\n",
    "            # replace comment character\n",
    "            body = body.replace(\"//\", \"#\")\n",
    "            # replace zombie line endings\n",
    "            body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "            # comment out these lines\n",
    "            body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "            body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "            fin.close()\n",
    "        # and write it to tmpfile\n",
    "        with open(tmpfile, 'w') as fout:\n",
    "            fout.write(body)\n",
    "        # Read again as Pandas dataframe\n",
    "        try:\n",
    "            df = pd.read_csv(tmpfile, sep=\",\", comment=\"#\", header=None, error_bad_lines=False, \n",
    "                             warn_bad_lines=True)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "        if len(df.columns) < 7:\n",
    "            continue            \n",
    "        try:\n",
    "            df[1] = pd.to_datetime(df[1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "        except:\n",
    "            dtmask = np.repeat(True,len(df))\n",
    "            for k in range(len(df)):\n",
    "                try:\n",
    "                    _ = pd.to_datetime(df.loc[k,1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "                except ValueError: \n",
    "                    dtmask[k] = False\n",
    "            df2 = df.loc[dtmask]\n",
    "            if len(df2)==0:\n",
    "                sys.exit()\n",
    "            df = df2\n",
    "            df[1] = pd.to_datetime(df[1], format=\"%Y/%m/%d %H:%M:%S\")\n",
    "        if len(df.columns) != numcols:\n",
    "                newnumcols.append(len(df.columns))\n",
    "                newcolsdate.append(df[1][0])\n",
    "                newcolsfile.append(name)\n",
    "                numcols=len(df.columns)\n",
    "    crns[id][\"setup\"] = pd.DataFrame({\"datetime\":newcolsdate, \"file\": newcolsfile, \"numcols\": newnumcols})\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "1\n",
      "             datetime                                               file  \\\n",
      "0 2019-07-25 10:39:58  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-04-15 10:23:28  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "2 2021-01-19 09:28:36  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "3 2021-01-25 12:02:02  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       10  \n",
      "1       15  \n",
      "2       14  \n",
      "3       15  \n",
      "\n",
      "======================================================\n",
      "2\n",
      "             datetime                                               file  \\\n",
      "0 2019-07-31 12:37:38  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-04-07 11:01:39  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "2 2021-04-19 10:39:18  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "3 2021-06-04 11:11:34  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0        8  \n",
      "1       15  \n",
      "2       10  \n",
      "3       15  \n",
      "\n",
      "======================================================\n",
      "4\n",
      "             datetime                                               file  \\\n",
      "0 2019-08-06 13:25:33  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-03-23 13:03:24  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       18  \n",
      "1       23  \n",
      "\n",
      "======================================================\n",
      "21\n",
      "             datetime                                               file  \\\n",
      "0 2019-07-18 09:58:42  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-04-07 10:05:40  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       13  \n",
      "1       20  \n",
      "\n",
      "======================================================\n",
      "22\n",
      "              datetime                                               file  \\\n",
      "0  2019-07-25 09:05:48  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1  2020-04-07 12:46:48  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "2  2020-07-08 10:53:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "3  2020-07-08 11:11:44  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "4  2020-07-08 13:55:22  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "5  2020-07-09 14:14:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "6  2020-07-09 20:14:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "7  2020-07-10 10:32:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "8  2020-09-25 09:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "9  2020-09-25 15:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "10 2020-10-21 15:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "11 2020-10-21 21:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "12 2020-10-24 21:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "13 2020-10-30 15:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "14 2020-10-30 21:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "15 2020-10-31 03:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "16 2020-10-31 09:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "17 2020-10-31 15:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "18 2020-11-01 15:34:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "19 2020-11-04 09:17:54  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "20 2020-11-10 15:37:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "21 2020-11-15 09:37:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "22 2020-11-20 15:37:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "23 2020-12-12 22:37:00  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "24 2021-01-25 12:40:40  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "25 2021-01-25 12:52:46  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "    numcols  \n",
      "0        13  \n",
      "1        20  \n",
      "2        17  \n",
      "3        28  \n",
      "4        29  \n",
      "5        28  \n",
      "6        29  \n",
      "7        27  \n",
      "8        23  \n",
      "9        27  \n",
      "10       24  \n",
      "11       27  \n",
      "12       23  \n",
      "13       27  \n",
      "14       23  \n",
      "15       27  \n",
      "16       23  \n",
      "17       27  \n",
      "18       23  \n",
      "19       26  \n",
      "20       27  \n",
      "21       26  \n",
      "22       27  \n",
      "23       26  \n",
      "24       17  \n",
      "25       20  \n",
      "\n",
      "======================================================\n",
      "26\n",
      "             datetime                                               file  \\\n",
      "0 2019-08-23 10:20:31  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-07-08 08:07:42  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "2 2020-07-08 08:23:28  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       11  \n",
      "1        7  \n",
      "2        9  \n",
      "\n",
      "======================================================\n",
      "27\n",
      "             datetime                                               file  \\\n",
      "0 2019-10-22 12:09:05  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2019-12-12 09:59:42  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       10  \n",
      "1       12  \n",
      "\n",
      "======================================================\n",
      "28\n",
      "             datetime                                               file  \\\n",
      "0 2019-11-21 11:09:47  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "1 2020-04-07 13:26:45  /home/maik/b2drop/cosmicsense/inbox/marquardt/...   \n",
      "\n",
      "   numcols  \n",
      "0       11  \n",
      "1       16  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in crns.keys():\n",
    "    print(\"======================================================\")\n",
    "    print(key)\n",
    "    print(crns[key][\"setup\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/maik/b2drop/cosmicsense/inbox/marquardt/timeseries/crns/remote/21/CRSProbe_Data2104010004.021_027577.txt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data 34234 doesn't match format specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1979\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1980\u001b[0m             \u001b[0;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2b1082981df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"34234\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%Y/%m/%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, box, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mallow_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         )\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data 34234 doesn't match format specified"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pd.to_datetime(\"34234\", format=\"%Y/%m/%d %H:%M:%S\")\n",
    "except TypeError:\n",
    "    print(\"ii\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(name, 'r') as file:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Processing 27\n",
      "Remote: found 329 files\n",
      ".........................................................................................................................................................................................................................................................................................................................................\n",
      "SD: found 768 files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "for i, id in enumerate(ids):\n",
    "    print(\"-------------\")\n",
    "    print(\"Processing %d\" % id)\n",
    "\n",
    "    try:\n",
    "        os.remove(tmpfile)\n",
    "        os.remove(tmpfile2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # REMOTE FILES\n",
    "    print(\"Remote: \", end=\"\")\n",
    "    searchdir = os.path.join(remotedir,\"%d\" % id, crns[id][\"remotepattern\"])\n",
    "    remotefiles = glob.glob(searchdir, recursive=True)\n",
    "    print(\"found %d files\" % len(remotefiles))\n",
    "\n",
    "    for name in remotefiles:\n",
    "        print(\".\", end=\"\")\n",
    "        fin = open(name, \"r\")\n",
    "        body = fin.read()\n",
    "        # replace comment character\n",
    "        body = body.replace(\"//\", \"#\")\n",
    "        # replace zombie line endings\n",
    "        body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "        # comment out these lines\n",
    "        body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "        body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "        myfile = open(tmpfile, 'a')\n",
    "        myfile.write(body)\n",
    "        myfile.close()\n",
    "    print(\"\")\n",
    "\n",
    "    # SD\n",
    "    print(\"SD: \", end=\"\")\n",
    "    searchdir = os.path.join(sddir, \"%d\" % id)\n",
    "    sdfiles = [filename for filename in Path(searchdir).glob(\"**/\"+crns[id][\"sdpattern\"])]\n",
    "    print(\"found %d files\" % len(sdfiles))\n",
    "\n",
    "    for name in sdfiles:\n",
    "        print(\".\", end=\"\")\n",
    "        fin = open(name, \"r\")\n",
    "        body = fin.read()\n",
    "        # replace comment character\n",
    "        body = body.replace(\"//\", \"#\")\n",
    "        # replace zombie line endings\n",
    "        body = body.replace(\",\\r\\n\", \"\\r\\n\")\n",
    "        body = body.replace(\",\\n\", \"\\n\")\n",
    "        # comment out these lines\n",
    "        body = body.replace(\"CRS#1:\", \"#CRS#1\")\n",
    "        body = body.replace(\"CRS#2:\", \"#CRS#2\")\n",
    "        myfile = open(tmpfile, 'a')\n",
    "        myfile.write(body)\n",
    "        myfile.close()\n",
    "    print(\"\")\n",
    "\n",
    "    if (\"colnames2\" in crns[id].keys()) or (\"colnames3\" in crns[id].keys()):\n",
    "        # Read all lines. potentially varying no of columns\n",
    "        myfile = open(tmpfile, 'r')\n",
    "        lines = myfile.readlines()\n",
    "        myfile.close()\n",
    "        # Write in seperate files\n",
    "        myfile = open(tmpfile, 'w')\n",
    "        myfile2 = open(tmpfile2, 'w')\n",
    "        myfile3 = open(tmpfile3, 'w')\n",
    "        for line in lines:\n",
    "            split = line.split(\",\")\n",
    "            if len(split)==len(crns[id][\"colnames\"]):\n",
    "                myfile.write(line+\"\\n\")\n",
    "            if len(split)==len(crns[id][\"colnames2\"]):\n",
    "                myfile2.write(line+\"\\n\")\n",
    "            try:\n",
    "                if len(split)==len(crns[id][\"colnames3\"]):\n",
    "                    myfile3.write(line+\"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "        myfile.close()\n",
    "        myfile2.close()\n",
    "        try:\n",
    "            myfile3.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # MERGE\n",
    "    df = pd.read_csv(tmpfile, sep=\",\", comment=\"#\", header=None, error_bad_lines=False, warn_bad_lines=True)\n",
    "    df.columns = crns[id][\"colnames\"]\n",
    "    if \"colnames2\" in crns[id].keys():\n",
    "        try:\n",
    "            df2 = pd.read_csv(tmpfile2, sep=\",\", comment=\"#\", header=None,\n",
    "                             error_bad_lines=False, warn_bad_lines=True)\n",
    "            df2.columns = crns[id][\"colnames2\"]\n",
    "            df = df2.append(df, sort=False)\n",
    "        except:\n",
    "            print(\"Problem in reading or appending data with diffferent column scenario\")\n",
    "            raise\n",
    "    if \"colnames3\" in crns[id].keys():\n",
    "        try:\n",
    "            df3 = pd.read_csv(tmpfile3, sep=\",\", comment=\"#\", header=None,\n",
    "                             error_bad_lines=False, warn_bad_lines=True)\n",
    "            df3.columns = crns[id][\"colnames3\"]\n",
    "            df = df3.append(df, sort=False)\n",
    "        except:\n",
    "            print(\"Problem in reading or appending data with diffferent column scenario\")\n",
    "            raise\n",
    "    df.datetime = pd.to_datetime(df.datetime, format=\"%Y/%m/%d %H:%M:%S\")\n",
    "    df = df.set_index(\"datetime\")\n",
    "    df.insert(loc=1, column=\"datetime\", value=df.index)\n",
    "    df = df.sort_index()\n",
    "    df = df[df.index >= \"2019-07-25\"]\n",
    "    dupl = df.index.duplicated(keep='first')\n",
    "    if np.any(dupl):\n",
    "        print(\"Contains %d duplicates\" % len(np.where(dupl)[0]))\n",
    "        df = df[~dupl]\n",
    "    fpath = os.path.join(trgdir, \"%d/%d_CRNS.txt\" % (id, id) )\n",
    "    df.to_csv(fpath, sep=\"\\t\", index=False, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
